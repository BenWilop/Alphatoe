{
    "transformer lens cfg": {
        "n_layers": 8,
        "d_model": 128,
        "n_ctx": 10,
        "d_head": 16,
        "model_name": "custom",
        "n_heads": 8,
        "d_mlp": 512,
        "act_fn": "relu",
        "d_vocab": 11,
        "eps": 1e-05,
        "use_attn_result": false,
        "use_attn_scale": true,
        "use_split_qkv_input": false,
        "use_local_attn": false,
        "original_architecture": null,
        "from_checkpoint": false,
        "checkpoint_index": null,
        "checkpoint_label_type": null,
        "checkpoint_value": null,
        "tokenizer_name": null,
        "window_size": null,
        "attn_types": null,
        "init_mode": "gpt2",
        "normalization_type": null,
        "device": "cuda",
        "n_devices": 1,
        "attention_dir": "causal",
        "attn_only": false,
        "seed": 1337,
        "initializer_range": 0.07071067811865475,
        "init_weights": true,
        "scale_attn_by_inverse_layer_idx": false,
        "positional_embedding_type": "standard",
        "final_rms": false,
        "d_vocab_out": 10,
        "parallel_attn_mlp": false,
        "rotary_dim": null,
        "n_params": 1572864,
        "use_hook_tokens": false,
        "gated_mlp": false
    },
    "other params": {
        "lr": 1e-05,
        "weight_decay": 0.0001,
        "test_train_split": 0.8,
        "epochs": 40,
        "batch_size": 4096
    }
}