{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from alphatoe import plot, game, interpretability\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import json\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "from showmethetypes import SMTT\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = SMTT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = interpretability.load_model(\n",
    "    \"../scripts/models/prob all 8 layer control-20230718-185339\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_attention(seq: torch.Tensor) -> torch.Tensor:\n",
    "    def hook(module, input, output):\n",
    "        module.attention = output.clone()\n",
    "\n",
    "    try:\n",
    "        handle = model.blocks[0].hook_attn_out.register_forward_hook(hook)\n",
    "        _ = model(seq)\n",
    "        attention = model.blocks[0].hook_attn_out.attention\n",
    "    except Exception as e:\n",
    "        handle.remove()\n",
    "        raise e\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_pre_mlp_residuals(seq: torch.Tensor, vec) -> torch.Tensor:\n",
    "    def hook(module, input, output):\n",
    "        return output + vec\n",
    "\n",
    "    try:\n",
    "        handle = model.blocks[0].hook_attn_out.register_forward_hook(hook)\n",
    "        out = model(seq)\n",
    "        handle.remove()\n",
    "    except Exception as e:\n",
    "        handle.remove()\n",
    "        raise e\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_vector(move: int):\n",
    "    moves = list(range(0, 9))\n",
    "    moves.remove(move)\n",
    "    z_seqs = [[10, move, snd] for snd in moves]\n",
    "    other_seqs = [[[10, fst, snd] for fst in moves if fst != snd] for snd in moves]\n",
    "    all_acts = []\n",
    "    flatten_acts = []\n",
    "    for i, z_seq in enumerate(z_seqs):\n",
    "        z_act = get_head_attention((torch.tensor(z_seq)))[0, -1]\n",
    "        z_acts = []\n",
    "        for other_seq in other_seqs[i]:\n",
    "            other_act = get_head_attention((torch.tensor(other_seq)))[0, -1]\n",
    "\n",
    "            act_diff = other_act - z_act\n",
    "            z_acts.append(act_diff)\n",
    "            flatten_acts.append(act_diff)\n",
    "\n",
    "        all_acts.append(z_acts)\n",
    "    flat_acts = torch.stack(flatten_acts)\n",
    "    # return torch.norm(flat_acts, dim=1)\n",
    "    return flat_acts.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(permutations(range(3), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_vector(l: int):\n",
    "    moves = list(range(0, 9))\n",
    "    seqs = torch.tensor(\n",
    "        list(map(lambda lis: [10] + list(lis), permutations(moves, l - 1)))\n",
    "    )\n",
    "    tt(seqs)\n",
    "\n",
    "    all_acts = get_head_attention(seqs)[:, -1]\n",
    "    # return torch.norm(flat_acts, dim=1)\n",
    "    tt(all_acts)\n",
    "    return all_acts.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = SMTT(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = SMTT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor (dtype: torch.int64)\n",
      "    |  (device: cpu)\n",
      "    |__dim_0 (15120)\n",
      "    |__dim_1 (6)\n",
      "Tensor (dtype: torch.float32)\n",
      "    |  (device: cuda:0)\n",
      "    |__dim_0 (15120)\n",
      "    |__dim_1 (128)\n",
      "Tensor (dtype: torch.float32)\n",
      "    |  (device: cuda:0)\n",
      "    |__dim_0 (128)\n"
     ]
    }
   ],
   "source": [
    "pos_vec = get_positional_vector(6)\n",
    "tt(pos_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_act = get_activation_vector(0)\n",
    "one_act = get_activation_vector(1)\n",
    "two_act = get_activation_vector(2)\n",
    "three_act = get_activation_vector(3)\n",
    "four_act = get_activation_vector(4)\n",
    "five_act = get_activation_vector(5)\n",
    "six_act = get_activation_vector(6)\n",
    "seven_act = get_activation_vector(7)\n",
    "eight_act = get_activation_vector(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor (dtype: torch.float32)\n",
      "    |  (device: cuda:0)\n",
      "    |__dim_0 (128)\n"
     ]
    }
   ],
   "source": [
    "tt(zero_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal output tensor([0.1111, 0.1112, 0.1113, 0.1109, 0.1113, 0.1112, 0.1110, 0.1107, 0.1113,\n",
      "        0.0000], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Subtracting 0, adding 1 tensor([ 38.1596,  38.6384,  40.7392,  -1.4404,  -2.3295,  -1.6009,  -3.1859,\n",
      "         -1.2897,  -1.2053, -90.7778], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Adding 0, subtracting 1 tensor([-24.3372, -16.9424, -20.2947,  25.0872,  25.0004,  22.9896,  24.7819,\n",
      "         23.7868,  22.5200, -65.9743], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq = [10]\n",
    "vec = zero_act + one_act + two_act\n",
    "out = model(torch.tensor(seq))\n",
    "add_vec = modify_pre_mlp_residuals(torch.tensor(seq), vec)\n",
    "subtract_vec = modify_pre_mlp_residuals(torch.tensor(seq), -vec)\n",
    "print(\"Normal output\", torch.nn.functional.softmax(out[0, -1], dim=0))\n",
    "print(\"Subtracting 0, adding 1\", add_vec[0, -1])\n",
    "print(\"Adding 0, subtracting 1\", subtract_vec[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_six = model.pos_embed(torch.tensor(range(6)))[-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor (dtype: torch.float32)\n",
      "    |  (device: cuda:0)\n",
      "    |__dim_0 (128)\n"
     ]
    }
   ],
   "source": [
    "tt(pos_six)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal output tensor([0.1111, 0.1112, 0.1113, 0.1109, 0.1113, 0.1112, 0.1110, 0.1107, 0.1113,\n",
      "        0.0000], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Subtracting 0, adding 1 tensor([  47.3994,   46.4811,   48.6058,   -2.3001,   -2.9395,   -0.8019,\n",
      "          -4.1709,   -2.7387,   -1.7201, -113.8362], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Adding 0, subtracting 1 tensor([-16.6672, -11.0680, -13.0832,  16.5233,  15.0852,  13.5191,  14.5390,\n",
      "         12.9728,  12.5428, -34.2667], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq = [10]\n",
    "vec = zero_act + one_act + two_act + pos_vec + pos_six\n",
    "out = model(torch.tensor(seq))\n",
    "add_vec = modify_pre_mlp_residuals(torch.tensor(seq), vec)\n",
    "subtract_vec = modify_pre_mlp_residuals(torch.tensor(seq), -vec)\n",
    "print(\"Normal output\", torch.nn.functional.softmax(out[0, -1], dim=0))\n",
    "print(\"Subtracting 0, adding 1\", add_vec[0, -1])\n",
    "print(\"Adding 0, subtracting 1\", subtract_vec[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  16.4406,   16.4415,   16.4427,   16.4385,   16.4421,   16.4415,\n",
       "            16.4397,   16.4371,   16.4422, -117.7469],\n",
       "         [ -73.5781,   13.6657,   13.6675,   13.6603,   13.6619,   13.6677,\n",
       "            13.6644,   13.6623,   13.6662,  -41.8547],\n",
       "         [ -28.0465,   19.5765,   14.9150,   16.1395,   14.9450,   16.9700,\n",
       "            15.4190,   17.5065,   15.6091,  -87.9268],\n",
       "         [ -30.0472,  -29.6935,   13.7019,   14.8522,   14.6196,   16.0425,\n",
       "            13.7595,   14.6526,   15.0848,  -41.4792],\n",
       "         [ -19.3795,  -10.7714,   14.2134,   15.0274,   14.2680,   15.9633,\n",
       "            14.2001,   17.0041,   14.0319,  -64.8916],\n",
       "         [ -33.0726,  -23.2774,  -34.1688,   10.0674,   10.0670,   10.6562,\n",
       "             9.0185,   10.3589,   10.4061,   18.4578]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [10, 0, 10, 1, 10, 2]\n",
    "vec = zero_act + one_act + two_act + pos_vec + pos_six\n",
    "out = model(torch.tensor(seq))\n",
    "add_vec = modify_pre_mlp_residuals(torch.tensor(seq), vec)\n",
    "subtract_vec = modify_pre_mlp_residuals(torch.tensor(seq), -vec)\n",
    "print(\"Normal output\", torch.nn.functional.softmax(out[0, -1], dim=0))\n",
    "print(\"Subtracting 0, adding 1\", add_vec[0, -1])\n",
    "print(\"Adding 0, subtracting 1\", subtract_vec[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10, 0, 1, 2] - zero_act - one_act - 2_act\n",
    "[10, 1, 2]\n",
    "[10, 2, 1]\n",
    "[10, 1, 2, 3]\n",
    "[10, 2, 1, 3]\n",
    "[10, 1, 3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
