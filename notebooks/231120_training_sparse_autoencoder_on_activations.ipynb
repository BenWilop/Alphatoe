{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from alphatoe import models, plot, interpretability, game\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_memlab import LineProfiler, MemReporter\n",
    "from showmethetypes import SMTT\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogFormatter\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = SMTT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc = models.SparseAutoEncoder(512, 512).to(\"cuda\")\n",
    "\n",
    "loss_fn = torch.nn.functional.mse_loss\n",
    "optimizer = torch.optim.Adam(autoenc.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_data = torch.load(\"all_games_act_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_data[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = loss_fn(torch.zeros(2, 2), torch.ones(2, 2), reduction=\"none\")\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a sparser encoder! (actually following instructions)\n",
    "- L0 around 10 or 20 on average across 1000 games\n",
    "- feature density is mostly under 1%\n",
    "- reconstruction loss stays low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 2**15\n",
    "lam = 1e-7\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, act_data.shape[0], batch_size):\n",
    "        dat = act_data[batch : batch + batch_size].to(\"cuda\")\n",
    "\n",
    "        l0, reg, guess = autoenc(dat)\n",
    "        mse_loss = loss_fn(guess, dat)\n",
    "\n",
    "        sparse_loss = lam * reg\n",
    "        # sparse_loss = 0\n",
    "        loss = mse_loss + sparse_loss\n",
    "        # losses.append(interpretability.numpy(loss))\n",
    "        losses.append([mse_loss.item(), sparse_loss.item(), l0.item()])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(losses[-1])\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_loss = loss_fn(guess, dat, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It should be noting that the 512 SAE seems to achieve the same loss and L0 than the 1024. Probably not worth looking into too much, but interesting observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_loss.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we've got an autoencoder, what do we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_freqs(num_batches=25, local_encoder=None):\n",
    "    if local_encoder is None:\n",
    "        local_encoder = encoder\n",
    "    act_freq_scores = torch.zeros(\n",
    "        local_encoder.W_in.shape[1], dtype=torch.float32\n",
    "    ).cuda()\n",
    "    total = 0\n",
    "    for i in tqdm.trange(num_batches):\n",
    "        tokens = act_data[torch.randperm(len(act_data))][: 2**14].to(\"cuda\")\n",
    "\n",
    "        hidden = local_encoder.get_act_density(tokens)\n",
    "\n",
    "        act_freq_scores += hidden\n",
    "        total += tokens.shape[0]\n",
    "    act_freq_scores /= total\n",
    "    num_dead = (act_freq_scores == 0).float().mean()\n",
    "    print(\"Num dead\", num_dead)\n",
    "    return act_freq_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = get_freqs(local_encoder=autoenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqs[112] * act_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = interpretability.numpy(freqs) * act_data.shape[0]\n",
    "# x = interpretability.numpy(freqs)\n",
    "x = x[np.isfinite(x)]\n",
    "fig, ax = plt.subplots()\n",
    "# set figure size\n",
    "fig.set_size_inches(10, 6)\n",
    "ax.hist(x, bins=np.logspace(np.log10(5), np.log10(10000000), 100))\n",
    "ax.set_xscale(\"log\")\n",
    "# x label\n",
    "# ax.xlabel(\"Number of Moves (log 10 scale)\");\n",
    "# y label\n",
    "# ax.ylabel(\"Count of Features(neuron acts)\");\n",
    "# set xtick and labels of ticks\n",
    "tick_positions = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "tick_labels = [\"1\", \"10\", \"100\", \"1k\", \"10k\", \"100k\", \"1M\"]\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels(tick_labels)\n",
    "# ax.get_xaxis().set_major_formatter(plt.ScalarFormatter());\n",
    "ax.set_xlabel(\"Number of Times Fired Out of 2,361,456\")\n",
    "ax.set_ylabel(\"Count of Features(neuron acts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    autoenc.state_dict(), \"./512_sparse_autoencoder_on_activations_20NOV2023_parameters.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
