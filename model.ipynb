{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import tqdm\n",
    "#functional\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from functools import partial\n",
    "import einops\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor, flat=False):\n",
    "    if type(tensor)!=torch.Tensor:\n",
    "        return tensor\n",
    "    if flat:\n",
    "        return tensor.flatten().detach().cpu().numpy()\n",
    "    else:\n",
    "        return tensor.detach().cpu().numpy()\n",
    "\n",
    "def imshow(tensor, xaxis=None, yaxis=None, animation_name='Snapshot', **kwargs):\n",
    "    tensor = torch.squeeze(tensor)\n",
    "    px.imshow(to_numpy(tensor, flat=False),aspect='auto', \n",
    "              labels={'x':xaxis, 'y':yaxis, 'animation_name':animation_name}, \n",
    "              **kwargs).show()\n",
    "# Set default colour scheme\n",
    "imshow = partial(imshow, color_continuous_scale='Blues')\n",
    "# Creates good defaults for showing divergent colour scales (ie with both \n",
    "# positive and negative values, where 0 is white)\n",
    "imshow_div = partial(imshow, color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "# Presets a bunch of defaults to imshow to make it suitable for showing heatmaps \n",
    "# of activations with x axis being input 1 and y axis being input 2.\n",
    "inputs_heatmap = partial(imshow, xaxis='Input 1', yaxis='Input 2', color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "\n",
    "def line(x, y=None, hover=None, xaxis='', yaxis='', **kwargs):\n",
    "    if type(y)==torch.Tensor:\n",
    "        y = to_numpy(y, flat=True)\n",
    "    if type(x)==torch.Tensor:\n",
    "        x = to_numpy(x, flat=True)\n",
    "    fig = px.line(x, y=y, hover_name=hover, **kwargs)\n",
    "    fig.update_layout(xaxis_title=xaxis, yaxis_title=yaxis)\n",
    "    fig.show()\n",
    "\n",
    "def lines(lines_list, x=None, mode='lines', labels=None, xaxis='', yaxis='', title = '', log_y=False, hover=None, **kwargs):\n",
    "    if type(lines_list)==torch.Tensor:\n",
    "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
    "    if x is None:\n",
    "        x=np.arange(len(lines_list[0]))\n",
    "    fig = go.Figure(layout={'title':title})\n",
    "    fig.update_xaxes(title=xaxis)\n",
    "    fig.update_yaxes(title=yaxis)\n",
    "    for c, line in enumerate(lines_list):\n",
    "        if type(line)==torch.Tensor:\n",
    "            line = to_numpy(line)\n",
    "        if labels is not None:\n",
    "            label = labels[c]\n",
    "        else:\n",
    "            label = c\n",
    "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
    "    if log_y:\n",
    "        fig.update_layout(yaxis_type=\"log\")\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Config Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 1,\n",
    "    d_model = 128,\n",
    "    d_head = 128,\n",
    "    d_mlp = 512,\n",
    "    act_fn = \"relu\",\n",
    "    normalization_type=None,\n",
    "    d_vocab=11,\n",
    "    d_vocab_out=10,\n",
    "    n_ctx=10,\n",
    "    init_weights=True,\n",
    "    device=\"cuda\",\n",
    "    seed = 1337,\n",
    ")\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "test_train_split = 0.8\n",
    "epochs = 100\n",
    "batch_size = 4096\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np_data = np.load('data/moves.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46080, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46080\n",
      "46080\n",
      "[10  0  1  2  3  4  6  5  8  7]\n",
      "[0 1 2 3 4 6 5 8 7 9]\n"
     ]
    }
   ],
   "source": [
    "#load npy file\n",
    "np_data = np.load('data/moves.npy')\n",
    "data = np_data[:, :-1]\n",
    "labels = np_data[:, 1:]\n",
    "\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "print(data[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 1, 0],\n",
      "         [0, 0, 0,  ..., 1, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 1, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 1]]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "encoded_labels = F.one_hot(t.tensor(labels))\n",
    "print(encoded_labels)\n",
    "print(t.sum(encoded_labels, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "encoded_data = F.one_hot(t.tensor(data))\n",
    "print(encoded_data[1238])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data and labels as numpy arrays\n",
    "data = np.array(data)\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "#data and encoded_labels as tensors\n",
    "data = t.from_numpy(data)\n",
    "encoded_labels = t.from_numpy(encoded_labels).to(t.float)\n",
    "total_data = list(zip(data, encoded_labels))\n",
    "split_data = list(t.utils.data.random_split(total_data, [.8, .2]))\n",
    "train_pairs = split_data[0]\n",
    "test_pairs= split_data[1]\n",
    "train_data, train_labels = zip(*train_pairs)\n",
    "test_data, test_labels = zip(*test_pairs)\n",
    "\n",
    "train_data = t.stack(train_data).to(cfg.device)\n",
    "train_labels = t.stack(train_labels).to(cfg.device)\n",
    "test_data = t.stack(test_data).to(cfg.device)\n",
    "test_labels = t.stack(test_labels).to(cfg.device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test train split\n",
    "train_data = data[:int(len(data)*test_train_split)]\n",
    "train_labels = encoded_labels[:int(len(data)*test_train_split)]\n",
    "test_data = data[int(len(data)*test_train_split):]\n",
    "test_labels = encoded_labels[int(len(data)*test_train_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9216\n",
      "9216\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    return t.nn.functional.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3133)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten = t.tensor([0,1]).to(t.float)\n",
    "loss_fn(ten, ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:43,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.301990032196045 | Test Loss: 2.2703514099121094\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.271068572998047 | Test Loss: 2.2395405769348145\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.23994779586792 | Test Loss: 2.2085189819335938\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.2092206478118896 | Test Loss: 2.1764636039733887\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.176811695098877 | Test Loss: 2.142791748046875\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.1434266567230225 | Test Loss: 2.10731840133667\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.1076362133026123 | Test Loss: 2.0705783367156982\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.070155382156372 | Test Loss: 2.0336356163024902\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 0 | Train Loss: 2.0340754985809326 | Test Loss: 1.9981554746627808\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.9980801343917847 | Test Loss: 1.965922474861145\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.9673292636871338 | Test Loss: 1.937679648399353\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.9375600814819336 | Test Loss: 1.9128265380859375\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.9140328168869019 | Test Loss: 1.8900750875473022\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.890297293663025 | Test Loss: 1.8686412572860718\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.8703263998031616 | Test Loss: 1.8483537435531616\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.8486641645431519 | Test Loss: 1.8294475078582764\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.8285305500030518 | Test Loss: 1.812081217765808\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 1 | Train Loss: 1.813177466392517 | Test Loss: 1.7959147691726685\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 2 | Train Loss: 1.7977007627487183 | Test Loss: 1.7800649404525757\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:17,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 1.7850780487060547 | Test Loss: 1.7634681463241577\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 2 | Train Loss: 1.7640475034713745 | Test Loss: 1.7454668283462524\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 2 | Train Loss: 1.7470600605010986 | Test Loss: 1.7260545492172241\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 2 | Train Loss: 1.7282990217208862 | Test Loss: 1.7056715488433838\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 2 | Train Loss: 1.707759141921997 | Test Loss: 1.6847436428070068\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 2 | Train Loss: 1.684143304824829 | Test Loss: 1.6626578569412231\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 2 | Train Loss: 1.6615384817123413 | Test Loss: 1.63853919506073\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 2 | Train Loss: 1.6412895917892456 | Test Loss: 1.61211359500885\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.615416407585144 | Test Loss: 1.583520531654358\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.589840292930603 | Test Loss: 1.5527479648590088\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.5554484128952026 | Test Loss: 1.519374966621399\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.5207288265228271 | Test Loss: 1.4836188554763794\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.4847495555877686 | Test Loss: 1.4458200931549072\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.447853684425354 | Test Loss: 1.406359076499939\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.4046601057052612 | Test Loss: 1.364904522895813\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.3644217252731323 | Test Loss: 1.3204925060272217\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 3 | Train Loss: 1.321488857269287 | Test Loss: 1.2731270790100098\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 4 | Train Loss: 1.273688554763794 | Test Loss: 1.22353994846344\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 4 | Train Loss: 1.2271744012832642 | Test Loss: 1.1708508729934692\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 4 | Train Loss: 1.1740831136703491 | Test Loss: 1.1150083541870117\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 4 | Train Loss: 1.1145362854003906 | Test Loss: 1.0571264028549194\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:00<00:12,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 1.055804967880249 | Test Loss: 0.9966414570808411\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 4 | Train Loss: 0.998585045337677 | Test Loss: 0.9336987137794495\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 4 | Train Loss: 0.9344625473022461 | Test Loss: 0.8701422810554504\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 4 | Train Loss: 0.8709927797317505 | Test Loss: 0.8066260814666748\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 4 | Train Loss: 0.8093199729919434 | Test Loss: 0.7441383600234985\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.745267391204834 | Test Loss: 0.6831145286560059\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.6857129335403442 | Test Loss: 0.6247608065605164\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.6265265345573425 | Test Loss: 0.5698306560516357\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.5715181231498718 | Test Loss: 0.5179246664047241\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.517713725566864 | Test Loss: 0.46841806173324585\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.468394011259079 | Test Loss: 0.4207397699356079\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.42351245880126953 | Test Loss: 0.3750752806663513\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.37480154633522034 | Test Loss: 0.33206844329833984\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 5 | Train Loss: 0.33606210350990295 | Test Loss: 0.291416198015213\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 6 | Train Loss: 0.291221559047699 | Test Loss: 0.25355300307273865\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 6 | Train Loss: 0.2551015317440033 | Test Loss: 0.21901148557662964\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 6 | Train Loss: 0.22010841965675354 | Test Loss: 0.18773473799228668\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 6 | Train Loss: 0.18895862996578217 | Test Loss: 0.15960176289081573\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 6 | Train Loss: 0.1602020114660263 | Test Loss: 0.13492557406425476\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 6 | Train Loss: 0.13458159565925598 | Test Loss: 0.11380327492952347\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 6 | Train Loss: 0.11567258834838867 | Test Loss: 0.09573955833911896\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:01<00:09,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.0959727093577385 | Test Loss: 0.08052349090576172\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 6 | Train Loss: 0.08155813813209534 | Test Loss: 0.06809712946414948\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.06833237409591675 | Test Loss: 0.05818776786327362\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.05873221904039383 | Test Loss: 0.04973116144537926\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.04990093410015106 | Test Loss: 0.04219410941004753\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.04259960353374481 | Test Loss: 0.035934869199991226\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.036075372248888016 | Test Loss: 0.031008217483758926\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.031222213059663773 | Test Loss: 0.027041928842663765\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.02733251266181469 | Test Loss: 0.023780616000294685\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.023842643946409225 | Test Loss: 0.02103719301521778\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 7 | Train Loss: 0.021040376275777817 | Test Loss: 0.01870683953166008\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.018808340653777122 | Test Loss: 0.016702858731150627\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.016962168738245964 | Test Loss: 0.014957020059227943\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.014978194609284401 | Test Loss: 0.01343599148094654\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.013585934415459633 | Test Loss: 0.012132631614804268\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.012280702590942383 | Test Loss: 0.011043697595596313\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.011148259043693542 | Test Loss: 0.010113473050296307\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.010222080163657665 | Test Loss: 0.00927244033664465\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.009298953227698803 | Test Loss: 0.008504824712872505\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 8 | Train Loss: 0.008500603027641773 | Test Loss: 0.007827955298125744\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 9 | Train Loss: 0.007950401864945889 | Test Loss: 0.007231445051729679\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:01<00:08, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.007358204107731581 | Test Loss: 0.00669760862365365\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 9 | Train Loss: 0.006756930146366358 | Test Loss: 0.006213771644979715\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 9 | Train Loss: 0.006286424584686756 | Test Loss: 0.005779371131211519\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 9 | Train Loss: 0.005878004245460033 | Test Loss: 0.0053921290673315525\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 9 | Train Loss: 0.005444241221994162 | Test Loss: 0.00504921842366457\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 9 | Train Loss: 0.005101026501506567 | Test Loss: 0.004745654296129942\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 9 | Train Loss: 0.004754047840833664 | Test Loss: 0.004474232438951731\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 9 | Train Loss: 0.004473147448152304 | Test Loss: 0.004229612648487091\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.0042833187617361546 | Test Loss: 0.004006821196526289\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.004071654286235571 | Test Loss: 0.003802781691774726\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.0038156979717314243 | Test Loss: 0.0036154550034552813\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.0036514431703835726 | Test Loss: 0.003442961722612381\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.0034920405596494675 | Test Loss: 0.0032842967193573713\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.0033131130039691925 | Test Loss: 0.0031389787327498198\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.0031851574312895536 | Test Loss: 0.003006163751706481\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.003001949517056346 | Test Loss: 0.0028854282572865486\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 10 | Train Loss: 0.0028868354856967926 | Test Loss: 0.002775507280603051\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 11 | Train Loss: 0.002803441369906068 | Test Loss: 0.0026750783436000347\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 11 | Train Loss: 0.0027193098794668913 | Test Loss: 0.0025827172212302685\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 11 | Train Loss: 0.0025895119179040194 | Test Loss: 0.002497381065040827\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 11 | Train Loss: 0.002522281603887677 | Test Loss: 0.0024180614855140448\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:01<00:08, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.0024464253801852465 | Test Loss: 0.00234392611309886\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 11 | Train Loss: 0.002365282503888011 | Test Loss: 0.0022742105647921562\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 11 | Train Loss: 0.0023117659147828817 | Test Loss: 0.0022089567501097918\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 11 | Train Loss: 0.0022096955217421055 | Test Loss: 0.0021481411531567574\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 11 | Train Loss: 0.0021499854046851397 | Test Loss: 0.002091736299917102\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.0021094840485602617 | Test Loss: 0.002039363607764244\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.002068719593808055 | Test Loss: 0.0019905806984752417\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.001994180493056774 | Test Loss: 0.0019450061954557896\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.001963801681995392 | Test Loss: 0.0019021208863705397\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.0019234666833654046 | Test Loss: 0.00186159648001194\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.0018742409301921725 | Test Loss: 0.0018231758149340749\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.001853539957664907 | Test Loss: 0.0017866413109004498\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.0017879009246826172 | Test Loss: 0.0017519420944154263\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 12 | Train Loss: 0.0017546433955430984 | Test Loss: 0.0017189469654113054\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 13 | Train Loss: 0.0017314881552010775 | Test Loss: 0.001687486656010151\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 13 | Train Loss: 0.0017078530509024858 | Test Loss: 0.001657444634474814\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 13 | Train Loss: 0.0016591619933024049 | Test Loss: 0.0016287794569507241\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 13 | Train Loss: 0.0016421641921624541 | Test Loss: 0.0016013813437893987\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 13 | Train Loss: 0.0016184309497475624 | Test Loss: 0.0015751435421407223\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 13 | Train Loss: 0.001583910547196865 | Test Loss: 0.001549951615743339\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 13 | Train Loss: 0.001575800939463079 | Test Loss: 0.0015257014892995358\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:01<00:07, 10.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.0015281018568202853 | Test Loss: 0.0015023474115878344\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 13 | Train Loss: 0.0015064658364281058 | Test Loss: 0.0014798303600400686\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.0014901889953762293 | Test Loss: 0.001458054524846375\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.001474240212701261 | Test Loss: 0.0014369753189384937\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.0014386108377948403 | Test Loss: 0.001416612882167101\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.0014276693109422922 | Test Loss: 0.0013969428837299347\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.0014114391524344683 | Test Loss: 0.0013779043219983578\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.0013845361536368728 | Test Loss: 0.0013594356132671237\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.0013822632608935237 | Test Loss: 0.0013415053253993392\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.0013434424763545394 | Test Loss: 0.0013240998378023505\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 14 | Train Loss: 0.001328883576206863 | Test Loss: 0.0013071896973997355\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.0013156587956473231 | Test Loss: 0.001290717045776546\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.001303959870710969 | Test Loss: 0.0012746559223160148\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.0012757342774420977 | Test Loss: 0.001258987351320684\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.0012688622809946537 | Test Loss: 0.0012436764081940055\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.0012565639335662127 | Test Loss: 0.0012287106364965439\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.0012337457155808806 | Test Loss: 0.0012140895705670118\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.0012346971780061722 | Test Loss: 0.0011997767724096775\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.00120096979662776 | Test Loss: 0.0011858086800202727\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 15 | Train Loss: 0.0011905819410458207 | Test Loss: 0.0011721622431650758\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 16 | Train Loss: 0.0011792590375989676 | Test Loss: 0.0011588024208322167\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:01<00:07, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 0.0011705746874213219 | Test Loss: 0.0011457238579168916\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 16 | Train Loss: 0.0011463674018159509 | Test Loss: 0.0011329231783747673\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 16 | Train Loss: 0.0011415740009397268 | Test Loss: 0.0011203923495486379\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 16 | Train Loss: 0.001132023986428976 | Test Loss: 0.001108107273466885\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 16 | Train Loss: 0.001112615573219955 | Test Loss: 0.0010960428044199944\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 16 | Train Loss: 0.0011146877659484744 | Test Loss: 0.0010841808980330825\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 16 | Train Loss: 0.001085318741388619 | Test Loss: 0.0010725517058745027\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 16 | Train Loss: 0.0010770575609058142 | Test Loss: 0.0010611464967951179\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.0010671202326193452 | Test Loss: 0.0010499432682991028\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.001060417271219194 | Test Loss: 0.0010389399249106646\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.0010392090771347284 | Test Loss: 0.0010281448485329747\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.0010360372252762318 | Test Loss: 0.0010175458155572414\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.0010282632429152727 | Test Loss: 0.0010071264114230871\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.0010110483272001147 | Test Loss: 0.0009968697559088469\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.0010136757045984268 | Test Loss: 0.000986766186542809\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.000987728824838996 | Test Loss: 0.000976838287897408\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 17 | Train Loss: 0.0009809726616367698 | Test Loss: 0.0009670790750533342\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 18 | Train Loss: 0.0009720977395772934 | Test Loss: 0.0009574713767506182\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 18 | Train Loss: 0.0009667680715210736 | Test Loss: 0.0009480128646828234\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 18 | Train Loss: 0.0009480101871304214 | Test Loss: 0.0009387098252773285\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 18 | Train Loss: 0.0009458890999667346 | Test Loss: 0.0009295524214394391\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:02<00:07, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 0.0009394578519277275 | Test Loss: 0.0009205344831570983\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 18 | Train Loss: 0.0009238924249075353 | Test Loss: 0.000911643379367888\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 18 | Train Loss: 0.0009269340080209076 | Test Loss: 0.0009028696804307401\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 18 | Train Loss: 0.0009037085110321641 | Test Loss: 0.0008942388813011348\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 18 | Train Loss: 0.0008980572456493974 | Test Loss: 0.0008857441716827452\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.0008900508400984108 | Test Loss: 0.0008773694862611592\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.0008857574430294335 | Test Loss: 0.0008691144757904112\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.0008689725073054433 | Test Loss: 0.000860988802742213\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.0008675504359416664 | Test Loss: 0.0008529868791811168\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.0008621698361821473 | Test Loss: 0.000845093687530607\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.0008480476099066436 | Test Loss: 0.000837305560708046\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.0008513244101777673 | Test Loss: 0.0008296119049191475\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.0008304223301820457 | Test Loss: 0.000822033325675875\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 19 | Train Loss: 0.00082563137402758 | Test Loss: 0.0008145676692947745\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 20 | Train Loss: 0.0008183288737200201 | Test Loss: 0.00080720434198156\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 20 | Train Loss: 0.0008147937478497624 | Test Loss: 0.0007999375811778009\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 20 | Train Loss: 0.0007997145876288414 | Test Loss: 0.0007927807746455073\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 20 | Train Loss: 0.0007988822762854397 | Test Loss: 0.000785722688306123\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 20 | Train Loss: 0.0007942959782667458 | Test Loss: 0.000778756570070982\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 20 | Train Loss: 0.000781369861215353 | Test Loss: 0.0007718782871961594\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 20 | Train Loss: 0.0007847623201087117 | Test Loss: 0.0007650773040950298\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:02<00:06, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 0.0007658340036869049 | Test Loss: 0.000758373353164643\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 20 | Train Loss: 0.0007617971277795732 | Test Loss: 0.0007517627673223615\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.000755111628677696 | Test Loss: 0.0007452411227859557\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.0007521556690335274 | Test Loss: 0.0007388003286905587\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.000738547823857516 | Test Loss: 0.0007324504549615085\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.0007381337927654386 | Test Loss: 0.000726187601685524\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.0007342075114138424 | Test Loss: 0.0007200014661066234\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.0007223595748655498 | Test Loss: 0.0007138896617107093\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.0007257675752043724 | Test Loss: 0.0007078427588567138\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.0007085629622451961 | Test Loss: 0.0007018781616352499\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 21 | Train Loss: 0.0007051440188661218 | Test Loss: 0.0006959927850402892\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.0006990195834077895 | Test Loss: 0.0006901820888742805\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.0006965243956074119 | Test Loss: 0.0006844415329396725\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.0006841927533969283 | Test Loss: 0.0006787788588553667\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.000684079306665808 | Test Loss: 0.0006731878966093063\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.0006807096651755273 | Test Loss: 0.0006676629418507218\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.0006698283250443637 | Test Loss: 0.0006621999200433493\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.0006731941248290241 | Test Loss: 0.0006567931268364191\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.000657488708384335 | Test Loss: 0.0006514590932056308\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 22 | Train Loss: 0.0006545586511492729 | Test Loss: 0.0006461917073465884\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 23 | Train Loss: 0.0006489493534900248 | Test Loss: 0.0006409883499145508\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:02<00:06, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 0.0006468213396146894 | Test Loss: 0.0006358455284498632\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 23 | Train Loss: 0.000635601463727653 | Test Loss: 0.000630769704002887\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 23 | Train Loss: 0.0006357257370837033 | Test Loss: 0.0006257587228901684\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 23 | Train Loss: 0.0006328379386104643 | Test Loss: 0.000620804843492806\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 23 | Train Loss: 0.0006228075362741947 | Test Loss: 0.000615903118159622\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 23 | Train Loss: 0.0006261146045289934 | Test Loss: 0.0006110514514148235\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 23 | Train Loss: 0.0006117022130638361 | Test Loss: 0.0006062620086595416\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 23 | Train Loss: 0.000609199982136488 | Test Loss: 0.0006015316466800869\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0006040645530447364 | Test Loss: 0.0005968562327325344\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0006022504530847073 | Test Loss: 0.0005922324489802122\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0005919968243688345 | Test Loss: 0.0005876714130863547\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0005923061980865896 | Test Loss: 0.0005831631715409458\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0005898429080843925 | Test Loss: 0.0005787057452835143\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0005805619293823838 | Test Loss: 0.0005742929060943425\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0005838011275045574 | Test Loss: 0.000569922907743603\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0005705235525965691 | Test Loss: 0.0005656053544953465\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 24 | Train Loss: 0.0005683886120095849 | Test Loss: 0.0005613414687104523\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 25 | Train Loss: 0.0005636804853565991 | Test Loss: 0.000557125371415168\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 25 | Train Loss: 0.000562127330340445 | Test Loss: 0.0005529549089260399\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 25 | Train Loss: 0.0005527297034859657 | Test Loss: 0.000548834097571671\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 25 | Train Loss: 0.0005531699862331152 | Test Loss: 0.0005447638686746359\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:02<00:06, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 0.0005510695627890527 | Test Loss: 0.0005407357239164412\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 25 | Train Loss: 0.0005424552364274859 | Test Loss: 0.0005367479170672596\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 25 | Train Loss: 0.0005456233047880232 | Test Loss: 0.0005327966646291316\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 25 | Train Loss: 0.0005333537119440734 | Test Loss: 0.0005288926186040044\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 25 | Train Loss: 0.0005315309972502291 | Test Loss: 0.0005250347894616425\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.000527199765201658 | Test Loss: 0.0005212196265347302\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.0005258712917566299 | Test Loss: 0.0005174422403797507\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.0005172318196855485 | Test Loss: 0.0005137121770530939\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.0005177754792384803 | Test Loss: 0.0005100247217342257\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.0005159850115887821 | Test Loss: 0.0005063759163022041\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.0005079704569652677 | Test Loss: 0.0005027607548981905\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.000511059770360589 | Test Loss: 0.0004991774912923574\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.0004996950156055391 | Test Loss: 0.0004956367774866521\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 26 | Train Loss: 0.0004981373785994947 | Test Loss: 0.0004921347717754543\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 27 | Train Loss: 0.000494147592689842 | Test Loss: 0.0004886719980277121\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 27 | Train Loss: 0.0004930117866024375 | Test Loss: 0.0004852437705267221\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 27 | Train Loss: 0.0004850531986448914 | Test Loss: 0.00048185570631176233\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 27 | Train Loss: 0.0004856671148445457 | Test Loss: 0.0004785051860380918\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 27 | Train Loss: 0.00048414740012958646 | Test Loss: 0.0004751876404043287\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 27 | Train Loss: 0.0004766701895277947 | Test Loss: 0.00047190109035000205\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 27 | Train Loss: 0.0004796783032361418 | Test Loss: 0.000468641024781391\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:02<00:06, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train Loss: 0.00046912222751416266 | Test Loss: 0.00046541940537281334\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 27 | Train Loss: 0.0004677890974562615 | Test Loss: 0.00046223122626543045\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.00046411162475124 | Test Loss: 0.0004590777098201215\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.00046313609345816076 | Test Loss: 0.0004559553926810622\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.00045578545541502535 | Test Loss: 0.00045286849490366876\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.0004564494302030653 | Test Loss: 0.0004498138732742518\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.00045515852980315685 | Test Loss: 0.0004467903927434236\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.00044817267917096615 | Test Loss: 0.0004437931929714978\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.00045109959319233894 | Test Loss: 0.0004408188397064805\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.00044126613647677004 | Test Loss: 0.000437877926742658\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 28 | Train Loss: 0.0004401266050990671 | Test Loss: 0.0004349696100689471\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.00043673402979038656 | Test Loss: 0.00043208911665715277\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.00043589179404079914 | Test Loss: 0.00042923673754557967\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.0004290817305445671 | Test Loss: 0.0004264171584509313\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.0004297848790884018 | Test Loss: 0.00042362604290246964\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.0004286973853595555 | Test Loss: 0.0004208609461784363\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.0004221529816277325 | Test Loss: 0.00041811863775365055\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.00042499788105487823 | Test Loss: 0.0004153990594204515\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.0004158135561738163 | Test Loss: 0.0004127078573219478\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 29 | Train Loss: 0.00041483977111056447 | Test Loss: 0.000410045322496444\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 30 | Train Loss: 0.0004117040953133255 | Test Loss: 0.00040740828262642026\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:03<00:06, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train Loss: 0.000410973938414827 | Test Loss: 0.0004047964175697416\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 30 | Train Loss: 0.00040465276106260717 | Test Loss: 0.00040221205563284457\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 30 | Train Loss: 0.00040537910535931587 | Test Loss: 0.00039965510950423777\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 30 | Train Loss: 0.00040447135688737035 | Test Loss: 0.0003971206024289131\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 30 | Train Loss: 0.00039832876063883305 | Test Loss: 0.0003946075157728046\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 30 | Train Loss: 0.0004010929842479527 | Test Loss: 0.0003921138704754412\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 30 | Train Loss: 0.0003924966440536082 | Test Loss: 0.000389645661925897\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 30 | Train Loss: 0.00039166491478681564 | Test Loss: 0.0003872033266816288\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.0003887642815243453 | Test Loss: 0.00038478177157230675\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.00038812780985608697 | Test Loss: 0.00038238507113419473\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.0003822518337983638 | Test Loss: 0.00038001281791366637\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.0003829921188298613 | Test Loss: 0.0003776629746425897\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.0003822420258074999 | Test Loss: 0.0003753348719328642\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.00037646389682777226 | Test Loss: 0.00037302623968571424\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.0003791486378759146 | Test Loss: 0.0003707318683154881\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.000371084752259776 | Test Loss: 0.0003684640978462994\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 31 | Train Loss: 0.0003703770926222205 | Test Loss: 0.0003662157978396863\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 32 | Train Loss: 0.0003676869091577828 | Test Loss: 0.00036398976226337254\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 32 | Train Loss: 0.0003671359736472368 | Test Loss: 0.00036178308073431253\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 32 | Train Loss: 0.00036165924393571913 | Test Loss: 0.0003595988964661956\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:03<00:05, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 | Train Loss: 0.00036240750341676176 | Test Loss: 0.00035743621992878616\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 32 | Train Loss: 0.00036179713788442314 | Test Loss: 0.0003552911221049726\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 32 | Train Loss: 0.0003563494828995317 | Test Loss: 0.0003531624097377062\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 32 | Train Loss: 0.0003589499683585018 | Test Loss: 0.0003510491515044123\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 32 | Train Loss: 0.00035137261147610843 | Test Loss: 0.00034895632416009903\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 32 | Train Loss: 0.0003507730725686997 | Test Loss: 0.00034688482992351055\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.0003482690663076937 | Test Loss: 0.00034483091440051794\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.0003477955178823322 | Test Loss: 0.00034279562532901764\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.00034268127637915313 | Test Loss: 0.000340779748512432\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.0003434311074670404 | Test Loss: 0.0003387836623005569\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.00034294271608814597 | Test Loss: 0.0003368027973920107\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.00033779931254684925 | Test Loss: 0.00033483767765574157\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.0003403176961001009 | Test Loss: 0.0003328874008730054\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.0003331846965011209 | Test Loss: 0.0003309535386506468\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 33 | Train Loss: 0.000332680472638458 | Test Loss: 0.0003290392633061856\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 34 | Train Loss: 0.0003303417470306158 | Test Loss: 0.0003271415189374238\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 34 | Train Loss: 0.000329941714880988 | Test Loss: 0.0003252580645494163\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 34 | Train Loss: 0.0003251537855248898 | Test Loss: 0.00032339521567337215\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 34 | Train Loss: 0.0003259000077378005 | Test Loss: 0.00032155014923773706\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 34 | Train Loss: 0.00032551976619288325 | Test Loss: 0.0003197167534381151\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 34 | Train Loss: 0.0003206559631507844 | Test Loss: 0.0003178993647452444\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:03<00:05, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 | Train Loss: 0.0003230949805583805 | Test Loss: 0.00031609315192326903\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 34 | Train Loss: 0.00031636565108783543 | Test Loss: 0.00031430349918082356\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 34 | Train Loss: 0.00031594614847563207 | Test Loss: 0.0003125302609987557\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.00031375783146359026 | Test Loss: 0.0003107719530817121\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.00031342310830950737 | Test Loss: 0.00030902816797606647\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.00030893218354322016 | Test Loss: 0.0003073005354963243\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.0003096718282904476 | Test Loss: 0.0003055886772926897\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.00030937898554839194 | Test Loss: 0.0003038910508621484\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.0003047810459975153 | Test Loss: 0.000302202592138201\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.00030713420710526407 | Test Loss: 0.0003005281905643642\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.0003007777559105307 | Test Loss: 0.000298868166282773\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 35 | Train Loss: 0.00030043229344300926 | Test Loss: 0.0002972227812279016\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.0002983839367516339 | Test Loss: 0.0002955911331810057\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.0002981042198371142 | Test Loss: 0.00029397165053524077\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.00029388206894509494 | Test Loss: 0.0002923674474004656\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.00029461688245646656 | Test Loss: 0.00029077869839966297\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.0002944006701000035 | Test Loss: 0.0002892013581003994\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.00029004650423303246 | Test Loss: 0.00028763432055711746\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.0002923195715993643 | Test Loss: 0.0002860776148736477\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.00028630546876229346 | Test Loss: 0.00028453487902879715\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 36 | Train Loss: 0.000286028312984854 | Test Loss: 0.0002830049197655171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:03<00:05, 11.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.00028410652885213494 | Test Loss: 0.0002814882027450949\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.0002838734944816679 | Test Loss: 0.00027998225414194167\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.0002798969217110425 | Test Loss: 0.0002784915850497782\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.0002806284464895725 | Test Loss: 0.0002770129940472543\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.0002804772520903498 | Test Loss: 0.0002755463938228786\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.0002763477386906743 | Test Loss: 0.00027408829191699624\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.00027854222571477294 | Test Loss: 0.00027263900847174227\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.00027284384123049676 | Test Loss: 0.0002712029963731766\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 37 | Train Loss: 0.0002726298407651484 | Test Loss: 0.00026977804373018444\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.00027082403539679945 | Test Loss: 0.00026836697361432016\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.0002706336963456124 | Test Loss: 0.000266965595073998\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.00026688427897170186 | Test Loss: 0.00026557472301647067\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.00026760660693980753 | Test Loss: 0.00026419784990139306\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.0002675143477972597 | Test Loss: 0.00026282991166226566\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.00026358963805250823 | Test Loss: 0.00026147073367610574\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.00026571014313958585 | Test Loss: 0.000260121509199962\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.0002603039611130953 | Test Loss: 0.0002587822964414954\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 38 | Train Loss: 0.0002601470332592726 | Test Loss: 0.0002574539103079587\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 39 | Train Loss: 0.00025844815536402166 | Test Loss: 0.00025613655452616513\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 39 | Train Loss: 0.00025829518563114107 | Test Loss: 0.00025482839555479586\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:03<00:05, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 | Train Loss: 0.00025474984431639314 | Test Loss: 0.0002535332168918103\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 39 | Train Loss: 0.0002554673992563039 | Test Loss: 0.0002522474096622318\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 39 | Train Loss: 0.00025542356888763607 | Test Loss: 0.0002509711775928736\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 39 | Train Loss: 0.0002516921085771173 | Test Loss: 0.0002497025125194341\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 39 | Train Loss: 0.0002537420659791678 | Test Loss: 0.00024844237486831844\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 39 | Train Loss: 0.00024860582198016346 | Test Loss: 0.00024719134671613574\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 39 | Train Loss: 0.00024849988403730094 | Test Loss: 0.00024595099966973066\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.00024689818383194506 | Test Loss: 0.00024472008226439357\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.0002467793528921902 | Test Loss: 0.00024349801242351532\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.00024342343385796994 | Test Loss: 0.00024228819529525936\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.00024413126811850816 | Test Loss: 0.00024108534853439778\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.0002441310789436102 | Test Loss: 0.00023989318287931383\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.00024057865084614605 | Test Loss: 0.00023870653240010142\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.00024255788594018668 | Test Loss: 0.00023752832203172147\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.00023767542734276503 | Test Loss: 0.00023635885736439377\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 40 | Train Loss: 0.00023761378542985767 | Test Loss: 0.00023519707610830665\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 41 | Train Loss: 0.00023610061907675117 | Test Loss: 0.00023404670355375856\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 41 | Train Loss: 0.00023601397697348148 | Test Loss: 0.00023290314129553735\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 41 | Train Loss: 0.00023283051268663257 | Test Loss: 0.00023176969261839986\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 41 | Train Loss: 0.0002335268072783947 | Test Loss: 0.0002306456008227542\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 41 | Train Loss: 0.0002335682074772194 | Test Loss: 0.00022952807194087654\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:04<00:05, 11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 | Train Loss: 0.00023018075444269925 | Test Loss: 0.0002284167130710557\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 41 | Train Loss: 0.00023209214850794524 | Test Loss: 0.00022731312492396683\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 41 | Train Loss: 0.00022744557645637542 | Test Loss: 0.00022621717653237283\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 41 | Train Loss: 0.00022742045985069126 | Test Loss: 0.0002251297264592722\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.0002259925240650773 | Test Loss: 0.0002240507019450888\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.00022593094035983086 | Test Loss: 0.000222981019760482\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.000222908376599662 | Test Loss: 0.00022191782773006707\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.00022359447029884905 | Test Loss: 0.00022086358512751758\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.00022366979101207107 | Test Loss: 0.00021981667669024318\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.00022043616627343 | Test Loss: 0.000218775006942451\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.00022228500165510923 | Test Loss: 0.00021774003107566386\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.00021785787248518318 | Test Loss: 0.00021671106514986604\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 42 | Train Loss: 0.00021786575962323695 | Test Loss: 0.00021569266391452402\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 43 | Train Loss: 0.00021651785937137902 | Test Loss: 0.00021468033082783222\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 43 | Train Loss: 0.00021647852554451674 | Test Loss: 0.00021367458975873888\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 43 | Train Loss: 0.0002136028342647478 | Test Loss: 0.0002126771432813257\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 43 | Train Loss: 0.00021427958563435823 | Test Loss: 0.0002116867690347135\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 43 | Train Loss: 0.00021438280236907303 | Test Loss: 0.0002107047475874424\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 43 | Train Loss: 0.00021129508968442678 | Test Loss: 0.00020972710626665503\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 43 | Train Loss: 0.00021308135183062404 | Test Loss: 0.00020875393238384277\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 43 | Train Loss: 0.00020885842968709767 | Test Loss: 0.00020778887846972793\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:04<00:04, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 | Train Loss: 0.00020889700681436807 | Test Loss: 0.0002068315225187689\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020762217172887176 | Test Loss: 0.00020588030747603625\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020760312327183783 | Test Loss: 0.00020493488409556448\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020486643188633025 | Test Loss: 0.00020399958884809166\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020553162903524935 | Test Loss: 0.00020306839724071324\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020566035527735949 | Test Loss: 0.00020214510732330382\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020270806271582842 | Test Loss: 0.00020122523710597306\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020443453104235232 | Test Loss: 0.00020031226449646056\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020040613890159875 | Test Loss: 0.0001994050689972937\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 44 | Train Loss: 0.00020046760619152337 | Test Loss: 0.00019850354874506593\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.0001992624020203948 | Test Loss: 0.00019761022122111171\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.00019926088862121105 | Test Loss: 0.00019672133203130215\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.0001966545096365735 | Test Loss: 0.00019584133406169713\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.0001973066246137023 | Test Loss: 0.00019496624008752406\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.00019745994359254837 | Test Loss: 0.00019409632659517229\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.00019463167700450867 | Test Loss: 0.00019323098240420222\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.00019630312453955412 | Test Loss: 0.00019237142987549305\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.00019245412840973586 | Test Loss: 0.00019151595188304782\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 45 | Train Loss: 0.00019253518257755786 | Test Loss: 0.00019066780805587769\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 46 | Train Loss: 0.00019139597134198993 | Test Loss: 0.0001898268674267456\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 46 | Train Loss: 0.00019141126540489495 | Test Loss: 0.00018898963753599674\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:04<00:04, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 | Train Loss: 0.00018892389198299497 | Test Loss: 0.00018816010560840368\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 46 | Train Loss: 0.00018956430722028017 | Test Loss: 0.00018733479373622686\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 46 | Train Loss: 0.00018973635451402515 | Test Loss: 0.0001865155209088698\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 46 | Train Loss: 0.00018702638044487685 | Test Loss: 0.00018570059910416603\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 46 | Train Loss: 0.00018864368030335754 | Test Loss: 0.00018488845671527088\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 46 | Train Loss: 0.000184961871127598 | Test Loss: 0.0001840836921473965\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 46 | Train Loss: 0.00018506291962694377 | Test Loss: 0.0001832839334383607\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.0001839831384131685 | Test Loss: 0.0001824907521950081\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.00018401276611257344 | Test Loss: 0.00018170078692492098\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.00018163624918088317 | Test Loss: 0.0001809180248528719\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.00018226401880383492 | Test Loss: 0.00018014013767242432\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.0001824552018661052 | Test Loss: 0.00017936722724698484\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.0001798549055820331 | Test Loss: 0.00017859786748886108\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.00018141807231586426 | Test Loss: 0.00017783221846912056\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.0001778965670382604 | Test Loss: 0.00017707253573462367\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 47 | Train Loss: 0.00017801379726734012 | Test Loss: 0.000176317582372576\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 48 | Train Loss: 0.00017698899318929762 | Test Loss: 0.0001755676930770278\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 48 | Train Loss: 0.00017703097546473145 | Test Loss: 0.00017482374096289277\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 48 | Train Loss: 0.00017475987260695547 | Test Loss: 0.0001740831066854298\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 48 | Train Loss: 0.0001753754186211154 | Test Loss: 0.0001733500830596313\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 48 | Train Loss: 0.00017558271065354347 | Test Loss: 0.0001726202026475221\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:04<00:04, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 | Train Loss: 0.00017308622773271054 | Test Loss: 0.00017189189384225756\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 48 | Train Loss: 0.00017459677474107593 | Test Loss: 0.0001711696822894737\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 48 | Train Loss: 0.00017122688586823642 | Test Loss: 0.00017045212734956294\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 48 | Train Loss: 0.00017135788220912218 | Test Loss: 0.00016973901074379683\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.00017038661462720484 | Test Loss: 0.0001690309145487845\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.0001704374299151823 | Test Loss: 0.00016832680557854474\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.0001682633883319795 | Test Loss: 0.0001676274259807542\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.00016886909725144506 | Test Loss: 0.00016693337238393724\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.00016908835095819086 | Test Loss: 0.0001662420982029289\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.00016668780881445855 | Test Loss: 0.00016555600450374186\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.0001681517605902627 | Test Loss: 0.00016487273387610912\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.00016492295253556222 | Test Loss: 0.00016419304301962256\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 49 | Train Loss: 0.0001650651392992586 | Test Loss: 0.00016351933300029486\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 50 | Train Loss: 0.00016414219862781465 | Test Loss: 0.00016285052697639912\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 50 | Train Loss: 0.00016420264728367329 | Test Loss: 0.00016218343807850033\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 50 | Train Loss: 0.00016212051559705287 | Test Loss: 0.00016152230091392994\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 50 | Train Loss: 0.00016271538333967328 | Test Loss: 0.0001608665188541636\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 50 | Train Loss: 0.0001629468606552109 | Test Loss: 0.0001602129195816815\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 50 | Train Loss: 0.00016063857765402645 | Test Loss: 0.00015956370043568313\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 50 | Train Loss: 0.00016205667634494603 | Test Loss: 0.00015891641669441015\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 50 | Train Loss: 0.0001589602034073323 | Test Loss: 0.0001582736149430275\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:05<00:04, 11.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 | Train Loss: 0.00015911283844616264 | Test Loss: 0.00015763497503940016\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.0001582356489961967 | Test Loss: 0.00015700106450822204\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.0001583019911777228 | Test Loss: 0.00015636993339285254\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.0001563067053211853 | Test Loss: 0.00015574412827845663\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.0001568914158269763 | Test Loss: 0.00015512316895183176\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.0001571332395542413 | Test Loss: 0.00015450383943971246\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.0001549104053992778 | Test Loss: 0.0001538883661851287\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.00015628435357939452 | Test Loss: 0.00015327564324252307\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.00015331286704167724 | Test Loss: 0.0001526666892459616\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 51 | Train Loss: 0.00015347350563388318 | Test Loss: 0.0001520612568128854\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.00015263927343767136 | Test Loss: 0.0001514599280199036\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.00015271086886059493 | Test Loss: 0.0001508630666648969\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.00015079951845109463 | Test Loss: 0.0001502690720371902\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.0001513726165285334 | Test Loss: 0.00014967971947044134\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.00015162226918619126 | Test Loss: 0.00014909377205185592\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.0001494823954999447 | Test Loss: 0.00014851041487418115\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.0001508150016888976 | Test Loss: 0.0001479279453633353\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.0001479599013691768 | Test Loss: 0.00014735069999005646\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 52 | Train Loss: 0.00014812688459642231 | Test Loss: 0.00014677834406029433\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 53 | Train Loss: 0.00014733619173057377 | Test Loss: 0.00014620661386288702\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 53 | Train Loss: 0.0001474116725148633 | Test Loss: 0.0001456402096664533\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:05<00:03, 11.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 | Train Loss: 0.0001455760357202962 | Test Loss: 0.00014507726882584393\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 53 | Train Loss: 0.00014613995153922588 | Test Loss: 0.00014451798051595688\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 53 | Train Loss: 0.00014639760775025934 | Test Loss: 0.0001439620100427419\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 53 | Train Loss: 0.000144334597280249 | Test Loss: 0.0001434069126844406\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 53 | Train Loss: 0.00014562490105163306 | Test Loss: 0.0001428549294359982\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 53 | Train Loss: 0.00014288185047917068 | Test Loss: 0.00014230776287149638\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 53 | Train Loss: 0.0001430543779861182 | Test Loss: 0.00014176327385939658\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.00014230083615984768 | Test Loss: 0.00014122112770564854\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.00014238145377021283 | Test Loss: 0.00014068307064007968\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.00014061776164453477 | Test Loss: 0.00014014898624736816\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.00014117239334154874 | Test Loss: 0.00013961701188236475\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.0001414365106029436 | Test Loss: 0.0001390880352118984\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.0001394440623698756 | Test Loss: 0.00013856159057468176\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.00014069740427657962 | Test Loss: 0.00013803664478473365\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.00013805933122057468 | Test Loss: 0.00013751622464042157\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 54 | Train Loss: 0.000138235351187177 | Test Loss: 0.00013699947157874703\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 55 | Train Loss: 0.00013751850929111242 | Test Loss: 0.0001364853815175593\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 55 | Train Loss: 0.0001376036088913679 | Test Loss: 0.00013597344513982534\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 55 | Train Loss: 0.00013590687012765557 | Test Loss: 0.00013546491391025484\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 55 | Train Loss: 0.0001364522468065843 | Test Loss: 0.00013495991879608482\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 55 | Train Loss: 0.00013672148634213954 | Test Loss: 0.00013445767399389297\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:05<00:03, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 | Train Loss: 0.00013479792687576264 | Test Loss: 0.00013395535643212497\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 55 | Train Loss: 0.00013601405953522772 | Test Loss: 0.00013345840852707624\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 55 | Train Loss: 0.00013347754429560155 | Test Loss: 0.00013296173710841686\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 55 | Train Loss: 0.00013365519407670945 | Test Loss: 0.0001324689801549539\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.00013297161785885692 | Test Loss: 0.0001319803559454158\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.0001330589147983119 | Test Loss: 0.00013149381265975535\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.0001314260734943673 | Test Loss: 0.00013101020886097103\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.0001319626608164981 | Test Loss: 0.00013052980648353696\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.00013223545101936907 | Test Loss: 0.0001300506992265582\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.0001303771568927914 | Test Loss: 0.00012957466242369264\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.0001315593544859439 | Test Loss: 0.00012910037185065448\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.000129115825984627 | Test Loss: 0.0001286291517317295\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 56 | Train Loss: 0.00012929728836752474 | Test Loss: 0.00012816049274988472\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 57 | Train Loss: 0.0001286463375436142 | Test Loss: 0.00012769544264301658\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 57 | Train Loss: 0.00012873606465291232 | Test Loss: 0.00012723154213745147\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 57 | Train Loss: 0.00012716064520645887 | Test Loss: 0.0001267706393264234\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 57 | Train Loss: 0.00012768949090968817 | Test Loss: 0.00012631391291506588\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 57 | Train Loss: 0.00012796740338671952 | Test Loss: 0.00012585839431267232\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 57 | Train Loss: 0.00012617082393262535 | Test Loss: 0.00012540401075966656\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 57 | Train Loss: 0.00012731879542116076 | Test Loss: 0.00012495268310885876\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 57 | Train Loss: 0.000124965314171277 | Test Loss: 0.00012450305803213269\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:05<00:03, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 | Train Loss: 0.00012514674745034426 | Test Loss: 0.00012405644520185888\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.00012452545342966914 | Test Loss: 0.00012361217522993684\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.00012461686856113374 | Test Loss: 0.00012317078653723001\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.00012309869634918869 | Test Loss: 0.0001227331958943978\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.00012361937842797488 | Test Loss: 0.000122297162306495\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.00012389985204208642 | Test Loss: 0.00012186289677629247\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.00012216343020554632 | Test Loss: 0.00012143031199229881\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.00012327880540397018 | Test Loss: 0.00012100039020879194\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.0001210101690958254 | Test Loss: 0.00012057104322593659\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 58 | Train Loss: 0.00012119189341319725 | Test Loss: 0.00012014565436402336\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00012059907021466643 | Test Loss: 0.00011972244101343676\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00012069315562257543 | Test Loss: 0.00011930118489544839\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00011922812700504437 | Test Loss: 0.00011888358858413994\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00011973911750828847 | Test Loss: 0.00011846734560094774\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00012002227595075965 | Test Loss: 0.00011805156827904284\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00011833906319225207 | Test Loss: 0.00011763966176658869\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00011942535638809204 | Test Loss: 0.00011722950875991955\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00011723684292519465 | Test Loss: 0.00011682001786539331\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 59 | Train Loss: 0.00011741912749130279 | Test Loss: 0.00011641505261650309\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 60 | Train Loss: 0.00011685430217767134 | Test Loss: 0.00011601150617934763\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 60 | Train Loss: 0.00011694955173879862 | Test Loss: 0.0001156078651547432\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:05<00:03, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 | Train Loss: 0.0001155335339717567 | Test Loss: 0.00011520938278408721\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 60 | Train Loss: 0.00011603676102822646 | Test Loss: 0.00011481294495752081\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 60 | Train Loss: 0.00011632197856670246 | Test Loss: 0.00011441731476224959\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 60 | Train Loss: 0.00011469283344922587 | Test Loss: 0.00011402335803722963\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 60 | Train Loss: 0.0001157494043582119 | Test Loss: 0.00011363098019501194\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 60 | Train Loss: 0.00011363637167960405 | Test Loss: 0.00011324074876029044\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 60 | Train Loss: 0.00011382007505744696 | Test Loss: 0.00011285315849818289\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.00011327788524795324 | Test Loss: 0.00011246812937315553\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.0001133746700361371 | Test Loss: 0.00011208317300770432\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.0001120075976359658 | Test Loss: 0.00011170162906637415\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.0001125007911468856 | Test Loss: 0.0001113230682676658\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.00011278777674306184 | Test Loss: 0.00011094583169324324\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.00011121023271698505 | Test Loss: 0.00011056895891670138\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.0001122385510825552 | Test Loss: 0.0001101940797525458\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.00011019755766028538 | Test Loss: 0.00010982179082930088\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 61 | Train Loss: 0.00011038122465834022 | Test Loss: 0.00010945076792268082\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 62 | Train Loss: 0.00010986238339683041 | Test Loss: 0.00010908194235526025\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 62 | Train Loss: 0.00010995881166309118 | Test Loss: 0.00010871562699321657\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 62 | Train Loss: 0.00010863868374144658 | Test Loss: 0.00010835030116140842\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 62 | Train Loss: 0.00010912407014984637 | Test Loss: 0.00010798784933285788\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 62 | Train Loss: 0.00010940999345621094 | Test Loss: 0.00010762727470137179\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:05<00:03, 11.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 | Train Loss: 0.00010788103099912405 | Test Loss: 0.00010726747859735042\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 62 | Train Loss: 0.00010888336692005396 | Test Loss: 0.00010690993804018945\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 62 | Train Loss: 0.00010691144416341558 | Test Loss: 0.0001065531323547475\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 62 | Train Loss: 0.00010709406342357397 | Test Loss: 0.00010619939712341875\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.00010659772669896483 | Test Loss: 0.00010584726260276511\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.0001066961485776119 | Test Loss: 0.00010549685976002365\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.00010542001109570265 | Test Loss: 0.000105148101283703\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.00010589690646156669 | Test Loss: 0.00010480011405888945\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.00010618211672408506 | Test Loss: 0.00010445644147694111\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.00010469984408700839 | Test Loss: 0.00010411145922262222\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.00010567633580649272 | Test Loss: 0.00010376901627751067\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.0001037681577145122 | Test Loss: 0.00010342831956222653\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 63 | Train Loss: 0.00010395183926448226 | Test Loss: 0.00010308927448932081\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 64 | Train Loss: 0.00010347592615289614 | Test Loss: 0.0001027525941026397\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 64 | Train Loss: 0.00010357455175835639 | Test Loss: 0.00010241592826787382\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 64 | Train Loss: 0.00010233847569907084 | Test Loss: 0.00010208332241745666\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 64 | Train Loss: 0.00010280816786689684 | Test Loss: 0.00010175068018725142\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 64 | Train Loss: 0.00010309489880455658 | Test Loss: 0.00010142206883756444\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 64 | Train Loss: 0.00010165637650061399 | Test Loss: 0.0001010914784274064\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 64 | Train Loss: 0.00010260617273161188 | Test Loss: 0.0001007636237773113\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 64 | Train Loss: 0.00010076124453917146 | Test Loss: 0.00010043707879958674\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:06<00:02, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 | Train Loss: 0.00010094331082655117 | Test Loss: 0.00010011216363636777\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 0.00010048670083051547 | Test Loss: 9.978978778235614e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 0.0001005862868623808 | Test Loss: 9.94682777673006e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 9.938951552612707e-05 | Test Loss: 9.914778638631105e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 9.985054202843457e-05 | Test Loss: 9.882986341835931e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 0.00010013727296609432 | Test Loss: 9.851455979514867e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 9.873984527075663e-05 | Test Loss: 9.819842671277002e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 9.966702054953203e-05 | Test Loss: 9.788454917725176e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 9.787989984033629e-05 | Test Loss: 9.757113002706319e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 65 | Train Loss: 9.806102752918378e-05 | Test Loss: 9.726065763970837e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.762388799572363e-05 | Test Loss: 9.695099288364872e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.772311750566587e-05 | Test Loss: 9.66433944995515e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.656447218731046e-05 | Test Loss: 9.633646550355479e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.701723320176825e-05 | Test Loss: 9.603229409549385e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.73042770056054e-05 | Test Loss: 9.57283700699918e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.594548464519903e-05 | Test Loss: 9.542668703943491e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.685056284070015e-05 | Test Loss: 9.512484393781051e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.511932148598135e-05 | Test Loss: 9.482490713708103e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 66 | Train Loss: 9.52991031226702e-05 | Test Loss: 9.452673111809418e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 67 | Train Loss: 9.48795277508907e-05 | Test Loss: 9.423040319234133e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 67 | Train Loss: 9.497906285105273e-05 | Test Loss: 9.393509390065446e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [00:06<00:02, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 | Train Loss: 9.385561861563474e-05 | Test Loss: 9.364181460114196e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 67 | Train Loss: 9.430235513718799e-05 | Test Loss: 9.334916830994189e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 67 | Train Loss: 9.45873252931051e-05 | Test Loss: 9.305899584433064e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 67 | Train Loss: 9.32676630327478e-05 | Test Loss: 9.27686269278638e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 67 | Train Loss: 9.41497492021881e-05 | Test Loss: 9.247891284758225e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 67 | Train Loss: 9.247234993381426e-05 | Test Loss: 9.219249477609992e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 67 | Train Loss: 9.265122207580134e-05 | Test Loss: 9.190665878122672e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 9.224807581631467e-05 | Test Loss: 9.162179776467383e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 9.234821482095867e-05 | Test Loss: 9.133891580859199e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 9.125890210270882e-05 | Test Loss: 9.105657954933122e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 9.169741679215804e-05 | Test Loss: 9.077702998183668e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 9.198264160659164e-05 | Test Loss: 9.049779328051955e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 9.069905354408547e-05 | Test Loss: 9.021856385516003e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 9.155816223938018e-05 | Test Loss: 8.994252129923552e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 8.993388473754749e-05 | Test Loss: 8.966594032244757e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 68 | Train Loss: 9.01096427696757e-05 | Test Loss: 8.939144754549488e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 69 | Train Loss: 8.972325304057449e-05 | Test Loss: 8.911819895729423e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 69 | Train Loss: 8.982264262158424e-05 | Test Loss: 8.884574344847351e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 69 | Train Loss: 8.8765453256201e-05 | Test Loss: 8.857509237714112e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 69 | Train Loss: 8.919710671762004e-05 | Test Loss: 8.830673323245719e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 69 | Train Loss: 8.948146569309756e-05 | Test Loss: 8.803904347587377e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:06<00:02, 11.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 | Train Loss: 8.823328244034201e-05 | Test Loss: 8.777180482866243e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 69 | Train Loss: 8.907149458536878e-05 | Test Loss: 8.750568667892367e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 69 | Train Loss: 8.749649714445695e-05 | Test Loss: 8.724067447474226e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 69 | Train Loss: 8.767056715441868e-05 | Test Loss: 8.697700104676187e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.729867840884253e-05 | Test Loss: 8.67146736709401e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.739794429857284e-05 | Test Loss: 8.645302295917645e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.637202699901536e-05 | Test Loss: 8.6194348114077e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.679823076818138e-05 | Test Loss: 8.593669190304354e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.708085806574672e-05 | Test Loss: 8.567737677367404e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.586419426137581e-05 | Test Loss: 8.542118303012103e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.668270311318338e-05 | Test Loss: 8.516534580849111e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.515497029293329e-05 | Test Loss: 8.491146581945941e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 70 | Train Loss: 8.532760693924502e-05 | Test Loss: 8.465755672659725e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 71 | Train Loss: 8.497003000229597e-05 | Test Loss: 8.440639066975564e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 71 | Train Loss: 8.506996528012678e-05 | Test Loss: 8.415467164013535e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 71 | Train Loss: 8.407289715250954e-05 | Test Loss: 8.39052299852483e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 71 | Train Loss: 8.44916285132058e-05 | Test Loss: 8.36564286146313e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 71 | Train Loss: 8.477184746880084e-05 | Test Loss: 8.340929343830794e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 71 | Train Loss: 8.358899503946304e-05 | Test Loss: 8.316228922922164e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 71 | Train Loss: 8.438771328656003e-05 | Test Loss: 8.291725680464879e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 71 | Train Loss: 8.290573896374553e-05 | Test Loss: 8.26726682134904e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:06<00:02, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 | Train Loss: 8.307619282277301e-05 | Test Loss: 8.242733747465536e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.27311523607932e-05 | Test Loss: 8.218600851250812e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.282998169306666e-05 | Test Loss: 8.194510155590251e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.186345075955614e-05 | Test Loss: 8.170521323336288e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.227559010265395e-05 | Test Loss: 8.14660670584999e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.255357533926144e-05 | Test Loss: 8.12283469713293e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.140203863149509e-05 | Test Loss: 8.099075785139576e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.218085713451728e-05 | Test Loss: 8.075494406512007e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.074261131696403e-05 | Test Loss: 8.051852637436241e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 72 | Train Loss: 8.090945630101487e-05 | Test Loss: 8.028510637814179e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 8.057924424065277e-05 | Test Loss: 8.005173003766686e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 8.067831367952749e-05 | Test Loss: 7.981985254446045e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 7.973733590915799e-05 | Test Loss: 7.958876813063398e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 8.014315244508907e-05 | Test Loss: 7.935972098493949e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 8.042050467338413e-05 | Test Loss: 7.91306301834993e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 7.92976570664905e-05 | Test Loss: 7.89026526035741e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 8.005924610188231e-05 | Test Loss: 7.867461681598797e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 7.866100349929184e-05 | Test Loss: 7.844873471185565e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 73 | Train Loss: 7.882807403802872e-05 | Test Loss: 7.822277984814718e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 74 | Train Loss: 7.850825932109728e-05 | Test Loss: 7.799967715982348e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:07<00:02, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 | Train Loss: 7.860839104978368e-05 | Test Loss: 7.77757159085013e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 74 | Train Loss: 7.769309013383463e-05 | Test Loss: 7.755433034617454e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 74 | Train Loss: 7.809328963048756e-05 | Test Loss: 7.733221718808636e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 74 | Train Loss: 7.836746226530522e-05 | Test Loss: 7.711263606324792e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 74 | Train Loss: 7.727299089310691e-05 | Test Loss: 7.689172343816608e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 74 | Train Loss: 7.801698666298762e-05 | Test Loss: 7.667347381357104e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 74 | Train Loss: 7.665866723982617e-05 | Test Loss: 7.645547884749249e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 74 | Train Loss: 7.682364230277017e-05 | Test Loss: 7.623796409461647e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.651581836398691e-05 | Test Loss: 7.602317782584578e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.66144585213624e-05 | Test Loss: 7.580833334941417e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.572530739707872e-05 | Test Loss: 7.559406367363408e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.611827459186316e-05 | Test Loss: 7.538139470852911e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.639094110345468e-05 | Test Loss: 7.51689076423645e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.532360177719966e-05 | Test Loss: 7.495813770219684e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.605203427374363e-05 | Test Loss: 7.47469239286147e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.473136065527797e-05 | Test Loss: 7.453673606505617e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 75 | Train Loss: 7.489421841455624e-05 | Test Loss: 7.432940765284002e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 76 | Train Loss: 7.459901098627597e-05 | Test Loss: 7.412122067762539e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 76 | Train Loss: 7.469661068171263e-05 | Test Loss: 7.391339750029147e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 76 | Train Loss: 7.383071351796389e-05 | Test Loss: 7.370692765107378e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [00:07<00:02, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 | Train Loss: 7.421689224429429e-05 | Test Loss: 7.350283703999594e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 76 | Train Loss: 7.448897667927667e-05 | Test Loss: 7.329812069656327e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 76 | Train Loss: 7.344682671828195e-05 | Test Loss: 7.309435022762045e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 76 | Train Loss: 7.415891013806686e-05 | Test Loss: 7.289140921784565e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 76 | Train Loss: 7.287473272299394e-05 | Test Loss: 7.268793706316501e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 76 | Train Loss: 7.303550228243694e-05 | Test Loss: 7.248748443089426e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.274961535586044e-05 | Test Loss: 7.228793401736766e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.284804451046512e-05 | Test Loss: 7.208852184703574e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.200597610790282e-05 | Test Loss: 7.188934250734746e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.23857301636599e-05 | Test Loss: 7.169215678004548e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.265424210345373e-05 | Test Loss: 7.149457087507471e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.163833652157336e-05 | Test Loss: 7.129891309887171e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.233552605612203e-05 | Test Loss: 7.110235310392454e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.10843742126599e-05 | Test Loss: 7.090784492902458e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 77 | Train Loss: 7.124624971766025e-05 | Test Loss: 7.071351137710735e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 78 | Train Loss: 7.096906483639032e-05 | Test Loss: 7.052087312331423e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 78 | Train Loss: 7.106604607542977e-05 | Test Loss: 7.032808935036883e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 78 | Train Loss: 7.024521619314328e-05 | Test Loss: 7.013633876340464e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 78 | Train Loss: 7.062072836561128e-05 | Test Loss: 6.994667637627572e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 78 | Train Loss: 7.088622805895284e-05 | Test Loss: 6.975589349167421e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [00:07<00:01, 10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 | Train Loss: 6.98945441399701e-05 | Test Loss: 6.956636207178235e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 78 | Train Loss: 7.057568291202188e-05 | Test Loss: 6.93779657012783e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 78 | Train Loss: 6.935915007488802e-05 | Test Loss: 6.91903114784509e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 78 | Train Loss: 6.951892282813787e-05 | Test Loss: 6.900267180753872e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.925214984221384e-05 | Test Loss: 6.881551234982908e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.934661359991878e-05 | Test Loss: 6.863150338176638e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.854895036667585e-05 | Test Loss: 6.844630115665495e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.891774683026597e-05 | Test Loss: 6.826176831964403e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.91800523782149e-05 | Test Loss: 6.807931640651077e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.821381248300895e-05 | Test Loss: 6.789638428017497e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.887941708555445e-05 | Test Loss: 6.771401240257546e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.769465107936412e-05 | Test Loss: 6.753300840500742e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 79 | Train Loss: 6.785267032682896e-05 | Test Loss: 6.735307397320867e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 80 | Train Loss: 6.759525422239676e-05 | Test Loss: 6.71722082188353e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 80 | Train Loss: 6.768861203454435e-05 | Test Loss: 6.699362711515278e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 80 | Train Loss: 6.691108137601987e-05 | Test Loss: 6.681502418359742e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 80 | Train Loss: 6.72740425216034e-05 | Test Loss: 6.663887324975803e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 80 | Train Loss: 6.753613706678152e-05 | Test Loss: 6.646040128543973e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 80 | Train Loss: 6.659032078459859e-05 | Test Loss: 6.628474511671811e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 80 | Train Loss: 6.724255945300683e-05 | Test Loss: 6.611046410398558e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 80 | Train Loss: 6.608951662201434e-05 | Test Loss: 6.593309808522463e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:07<00:01, 11.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 | Train Loss: 6.624406523769721e-05 | Test Loss: 6.575923907803372e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.599527841899544e-05 | Test Loss: 6.558666791534051e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.608924741158262e-05 | Test Loss: 6.541279435623437e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.533063424285501e-05 | Test Loss: 6.524119817186147e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.568824028363451e-05 | Test Loss: 6.506993668153882e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.594687147298828e-05 | Test Loss: 6.490003579529002e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.502569158328697e-05 | Test Loss: 6.472897075582296e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.566262163687497e-05 | Test Loss: 6.455900438595563e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.453745299950242e-05 | Test Loss: 6.438972195610404e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 81 | Train Loss: 6.469259096775204e-05 | Test Loss: 6.422298611141741e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.4452899096068e-05 | Test Loss: 6.40547732473351e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.45443651592359e-05 | Test Loss: 6.388621841324493e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.38034543953836e-05 | Test Loss: 6.372118514264002e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.415706593543291e-05 | Test Loss: 6.355615187203512e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.441424193326384e-05 | Test Loss: 6.338992534438148e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.35113101452589e-05 | Test Loss: 6.322610715869814e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.413574010366574e-05 | Test Loss: 6.306219438556582e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.304005364654586e-05 | Test Loss: 6.289935845416039e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 82 | Train Loss: 6.319398380583152e-05 | Test Loss: 6.273452891036868e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 83 | Train Loss: 6.295914499787614e-05 | Test Loss: 6.257386121433228e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 83 | Train Loss: 6.305065471678972e-05 | Test Loss: 6.241184019017965e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [00:07<00:01, 11.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 | Train Loss: 6.232900341274217e-05 | Test Loss: 6.225262768566608e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 83 | Train Loss: 6.267743447097018e-05 | Test Loss: 6.209143612068146e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 83 | Train Loss: 6.293020851444453e-05 | Test Loss: 6.193338049342856e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 83 | Train Loss: 6.205063255038112e-05 | Test Loss: 6.17737096035853e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 83 | Train Loss: 6.266161653911695e-05 | Test Loss: 6.1615755839739e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 83 | Train Loss: 6.159243639558554e-05 | Test Loss: 6.145717634353787e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 83 | Train Loss: 6.174383452162147e-05 | Test Loss: 6.13012962276116e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.152060814201832e-05 | Test Loss: 6.114276038715616e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.160731572890654e-05 | Test Loss: 6.09880116826389e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.0904480051249266e-05 | Test Loss: 6.0832542658317834e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.124709034338593e-05 | Test Loss: 6.067817594157532e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.1498794821091e-05 | Test Loss: 6.052486423868686e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.063778346288018e-05 | Test Loss: 6.037066486896947e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.123663479229435e-05 | Test Loss: 6.021603985573165e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.019217107677832e-05 | Test Loss: 6.0064725403208286e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 84 | Train Loss: 6.0344140365486965e-05 | Test Loss: 5.991220677969977e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 85 | Train Loss: 6.0125708841951564e-05 | Test Loss: 5.976149986963719e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 85 | Train Loss: 6.021382432663813e-05 | Test Loss: 5.961009082966484e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 85 | Train Loss: 5.95268102188129e-05 | Test Loss: 5.946042438154109e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 85 | Train Loss: 5.9864858485525474e-05 | Test Loss: 5.9311481891199946e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 85 | Train Loss: 6.0114263760624453e-05 | Test Loss: 5.916235750191845e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [00:08<00:01, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 | Train Loss: 5.9271755162626505e-05 | Test Loss: 5.901376425754279e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 85 | Train Loss: 5.9858051827177405e-05 | Test Loss: 5.8864236052613705e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 85 | Train Loss: 5.8838904806179926e-05 | Test Loss: 5.871767643839121e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 85 | Train Loss: 5.898890594835393e-05 | Test Loss: 5.857040741830133e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.877863077330403e-05 | Test Loss: 5.842433893121779e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.886489088879898e-05 | Test Loss: 5.827758286613971e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.8194105804432184e-05 | Test Loss: 5.813304233015515e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.8528130466584116e-05 | Test Loss: 5.798911661258899e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.877532021258958e-05 | Test Loss: 5.784438326372765e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.795078686787747e-05 | Test Loss: 5.7700050092535093e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.852470712852664e-05 | Test Loss: 5.7557681429898366e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.7531229685992e-05 | Test Loss: 5.7415007177041844e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 86 | Train Loss: 5.7680044847074896e-05 | Test Loss: 5.727259485865943e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 87 | Train Loss: 5.7475710491416976e-05 | Test Loss: 5.713083010050468e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 87 | Train Loss: 5.756008977186866e-05 | Test Loss: 5.698936729459092e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 87 | Train Loss: 5.690605757990852e-05 | Test Loss: 5.685014548362233e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 87 | Train Loss: 5.723576759919524e-05 | Test Loss: 5.670926839229651e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 87 | Train Loss: 5.747838440584019e-05 | Test Loss: 5.65696791454684e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 87 | Train Loss: 5.6671906349947676e-05 | Test Loss: 5.643088661599904e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 87 | Train Loss: 5.723509821109474e-05 | Test Loss: 5.629241422866471e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [00:08<00:00, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 | Train Loss: 5.626518031931482e-05 | Test Loss: 5.61540546186734e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 87 | Train Loss: 5.6412332924082875e-05 | Test Loss: 5.6016349844867364e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.6214277719845995e-05 | Test Loss: 5.587952182395384e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.629828592645936e-05 | Test Loss: 5.5742275435477495e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.565887113334611e-05 | Test Loss: 5.56067461729981e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.598340794676915e-05 | Test Loss: 5.5471416999353096e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.622449316433631e-05 | Test Loss: 5.533589865081012e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.543442603084259e-05 | Test Loss: 5.520130071090534e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.598719508270733e-05 | Test Loss: 5.506748129846528e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.5039996368577704e-05 | Test Loss: 5.493345815921202e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 88 | Train Loss: 5.518439138540998e-05 | Test Loss: 5.480086474562995e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.499413964571431e-05 | Test Loss: 5.466671791509725e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.507491368916817e-05 | Test Loss: 5.4535645176656544e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.4452371841762215e-05 | Test Loss: 5.440476525109261e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.4772128351032734e-05 | Test Loss: 5.427300129667856e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.500989573192783e-05 | Test Loss: 5.41422632522881e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.423790935310535e-05 | Test Loss: 5.401071030064486e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.477888771565631e-05 | Test Loss: 5.3881762141827494e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.385379699873738e-05 | Test Loss: 5.3751191444462165e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 89 | Train Loss: 5.399627843871713e-05 | Test Loss: 5.3623287385562435e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.3811760153621435e-05 | Test Loss: 5.349344064597972e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [00:08<00:00, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.3891355491941795e-05 | Test Loss: 5.336624235496856e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.3282052249414846e-05 | Test Loss: 5.3238611144479364e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.3598032536683604e-05 | Test Loss: 5.3111041779629886e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.383332245401107e-05 | Test Loss: 5.298515679896809e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.30778088432271e-05 | Test Loss: 5.285854422254488e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.360872091841884e-05 | Test Loss: 5.2732459153048694e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.2702973334817216e-05 | Test Loss: 5.260774923954159e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 90 | Train Loss: 5.2846404287265614e-05 | Test Loss: 5.248177330940962e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.2666349802166224e-05 | Test Loss: 5.2356688684085384e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.274572686175816e-05 | Test Loss: 5.223318294156343e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.2149513066979125e-05 | Test Loss: 5.2109982789261267e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.246133150649257e-05 | Test Loss: 5.1985945901833475e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.2693369070766494e-05 | Test Loss: 5.1864171837223694e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.1953607908217236e-05 | Test Loss: 5.174168472876772e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.247439185041003e-05 | Test Loss: 5.161904118722305e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.158861677045934e-05 | Test Loss: 5.149667413206771e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 91 | Train Loss: 5.172856253921054e-05 | Test Loss: 5.1376267947489396e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 92 | Train Loss: 5.155688995728269e-05 | Test Loss: 5.1255196012789384e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 92 | Train Loss: 5.163446985534392e-05 | Test Loss: 5.1135706598870456e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [00:08<00:00, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 | Train Loss: 5.105164018459618e-05 | Test Loss: 5.101603164803237e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 92 | Train Loss: 5.135930405231193e-05 | Test Loss: 5.0896447646664456e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 92 | Train Loss: 5.158887506695464e-05 | Test Loss: 5.0776889111148193e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 92 | Train Loss: 5.0863975047832355e-05 | Test Loss: 5.065797449788079e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 92 | Train Loss: 5.1374907343415543e-05 | Test Loss: 5.0540165830170736e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 92 | Train Loss: 5.050955951446667e-05 | Test Loss: 5.042129851062782e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 92 | Train Loss: 5.064783545094542e-05 | Test Loss: 5.030533793615177e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 5.0481397920520976e-05 | Test Loss: 5.018713272875175e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 5.055640576756559e-05 | Test Loss: 5.006990613765083e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 4.998631993657909e-05 | Test Loss: 4.995435301680118e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 5.0290487706661224e-05 | Test Loss: 4.983928010915406e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 5.051810512668453e-05 | Test Loss: 4.9723614210961387e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 4.98075460200198e-05 | Test Loss: 4.9607919208938256e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 5.030847023590468e-05 | Test Loss: 4.949361755279824e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 4.946239278069697e-05 | Test Loss: 4.9379017582396045e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 93 | Train Loss: 4.959994839737192e-05 | Test Loss: 4.926435212837532e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 94 | Train Loss: 4.943623207509518e-05 | Test Loss: 4.915124372928403e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 94 | Train Loss: 4.951289156451821e-05 | Test Loss: 4.903860462945886e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 94 | Train Loss: 4.895469101029448e-05 | Test Loss: 4.8925492592388764e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 94 | Train Loss: 4.925378743791953e-05 | Test Loss: 4.881285349256359e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [00:08<00:00, 10.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 | Train Loss: 4.9478327127872035e-05 | Test Loss: 4.8700003389967605e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 94 | Train Loss: 4.87815668748226e-05 | Test Loss: 4.859014370595105e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 94 | Train Loss: 4.9275255150860175e-05 | Test Loss: 4.84778756799642e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 94 | Train Loss: 4.844596332986839e-05 | Test Loss: 4.8365298425778747e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 94 | Train Loss: 4.858102693106048e-05 | Test Loss: 4.825624273507856e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.8424219130538404e-05 | Test Loss: 4.814544445252977e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.8499197873752564e-05 | Test Loss: 4.803640331374481e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.7953602916095406e-05 | Test Loss: 4.792683830601163e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.8247802624246106e-05 | Test Loss: 4.781710231327452e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.846909359912388e-05 | Test Loss: 4.770937084686011e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.778803850058466e-05 | Test Loss: 4.760175943374634e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.827157317777164e-05 | Test Loss: 4.749251820612699e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.7460209316341206e-05 | Test Loss: 4.738548523164354e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 95 | Train Loss: 4.759611329063773e-05 | Test Loss: 4.7277691919589415e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 96 | Train Loss: 4.744193938677199e-05 | Test Loss: 4.717012416222133e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 96 | Train Loss: 4.751505912281573e-05 | Test Loss: 4.706411709776148e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 96 | Train Loss: 4.698144766734913e-05 | Test Loss: 4.695865209214389e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 96 | Train Loss: 4.727340638055466e-05 | Test Loss: 4.6852510422468185e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 96 | Train Loss: 4.749152503791265e-05 | Test Loss: 4.674650699598715e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 96 | Train Loss: 4.682282451540232e-05 | Test Loss: 4.664259176934138e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 96 | Train Loss: 4.729807915282436e-05 | Test Loss: 4.653623545891605e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [00:09<00:00, 11.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 | Train Loss: 4.6502846089424565e-05 | Test Loss: 4.643200009013526e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 96 | Train Loss: 4.663770232582465e-05 | Test Loss: 4.632792479242198e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.648849062505178e-05 | Test Loss: 4.622337291948497e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.656014425563626e-05 | Test Loss: 4.612081102095544e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.603829074767418e-05 | Test Loss: 4.601804539561272e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.6326244046213105e-05 | Test Loss: 4.591475226334296e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.6541401388822123e-05 | Test Loss: 4.5811910240445286e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.588545925798826e-05 | Test Loss: 4.5709151891060174e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.635014192899689e-05 | Test Loss: 4.560881279758178e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.557508509606123e-05 | Test Loss: 4.5507018512580544e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 97 | Train Loss: 4.570792953018099e-05 | Test Loss: 4.540707959677093e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.556403655442409e-05 | Test Loss: 4.5304757804842666e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.5633543777512386e-05 | Test Loss: 4.520450602285564e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.51212799816858e-05 | Test Loss: 4.510373037192039e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.540538429864682e-05 | Test Loss: 4.500426075537689e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.5619421143783256e-05 | Test Loss: 4.490549326874316e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.4976233766647056e-05 | Test Loss: 4.480564166442491e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.5432938350131735e-05 | Test Loss: 4.470654312171973e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.467231337912381e-05 | Test Loss: 4.4608521420741454e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 98 | Train Loss: 4.4805212382925674e-05 | Test Loss: 4.451032873475924e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 99 | Train Loss: 4.466392783797346e-05 | Test Loss: 4.441173223312944e-05\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 | Train Loss: 4.473371882340871e-05 | Test Loss: 4.431409979588352e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 99 | Train Loss: 4.423230348038487e-05 | Test Loss: 4.421750418259762e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 99 | Train Loss: 4.451301720109768e-05 | Test Loss: 4.411974805407226e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 99 | Train Loss: 4.4723052269546315e-05 | Test Loss: 4.4024262024322525e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 99 | Train Loss: 4.409309622133151e-05 | Test Loss: 4.392766277305782e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 99 | Train Loss: 4.45414443674963e-05 | Test Loss: 4.38314164057374e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 99 | Train Loss: 4.379630263429135e-05 | Test Loss: 4.3735602957895026e-05\n",
      "torch.float32\n",
      "torch.float32\n",
      "Epoch 99 | Train Loss: 4.39273972006049e-05 | Test Loss: 4.364071719464846e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "model = HookedTransformer(cfg).to(cfg.device)\n",
    "optimizer = t.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# for epoch in tqdm.tqdm(range(epochs)):\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, len(train_data), batch_size):\n",
    "        train_logits = model(train_data[batch:batch+batch_size])\n",
    "        print(train_logits.dtype)\n",
    "        print(train_labels.dtype)\n",
    "        train_loss = loss_fn(train_logits, train_labels[batch:batch+batch_size])\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with t.inference_mode():\n",
    "            test_logits = model(test_data)\n",
    "            test_loss = loss_fn(test_logits, test_labels)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss.item()} | Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = t.tensor([10] * 10).to(cfg.device)\n",
    "out = model(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-68.5691, -53.3248, -33.7109,  28.4673, -47.1521,  49.8403,  20.7787,\n",
       "          18.6870, -19.6562,  14.9037]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899
         ],
         "xaxis": "x",
         "y": [
          2.2703514099121094,
          2.2395405769348145,
          2.2085189819335938,
          2.1764636039733887,
          2.142791748046875,
          2.10731840133667,
          2.0705783367156982,
          2.0336356163024902,
          1.9981554746627808,
          1.965922474861145,
          1.937679648399353,
          1.9128265380859375,
          1.8900750875473022,
          1.8686412572860718,
          1.8483537435531616,
          1.8294475078582764,
          1.812081217765808,
          1.7959147691726685,
          1.7800649404525757,
          1.7634681463241577,
          1.7454668283462524,
          1.7260545492172241,
          1.7056715488433838,
          1.6847436428070068,
          1.6626578569412231,
          1.63853919506073,
          1.61211359500885,
          1.583520531654358,
          1.5527479648590088,
          1.519374966621399,
          1.4836188554763794,
          1.4458200931549072,
          1.406359076499939,
          1.364904522895813,
          1.3204925060272217,
          1.2731270790100098,
          1.22353994846344,
          1.1708508729934692,
          1.1150083541870117,
          1.0571264028549194,
          0.9966414570808411,
          0.9336987137794495,
          0.8701422810554504,
          0.8066260814666748,
          0.7441383600234985,
          0.6831145286560059,
          0.6247608065605164,
          0.5698306560516357,
          0.5179246664047241,
          0.46841806173324585,
          0.4207397699356079,
          0.3750752806663513,
          0.33206844329833984,
          0.291416198015213,
          0.25355300307273865,
          0.21901148557662964,
          0.18773473799228668,
          0.15960176289081573,
          0.13492557406425476,
          0.11380327492952347,
          0.09573955833911896,
          0.08052349090576172,
          0.06809712946414948,
          0.05818776786327362,
          0.04973116144537926,
          0.04219410941004753,
          0.035934869199991226,
          0.031008217483758926,
          0.027041928842663765,
          0.023780616000294685,
          0.02103719301521778,
          0.01870683953166008,
          0.016702858731150627,
          0.014957020059227943,
          0.01343599148094654,
          0.012132631614804268,
          0.011043697595596313,
          0.010113473050296307,
          0.00927244033664465,
          0.008504824712872505,
          0.007827955298125744,
          0.007231445051729679,
          0.00669760862365365,
          0.006213771644979715,
          0.005779371131211519,
          0.0053921290673315525,
          0.00504921842366457,
          0.004745654296129942,
          0.004474232438951731,
          0.004229612648487091,
          0.004006821196526289,
          0.003802781691774726,
          0.0036154550034552813,
          0.003442961722612381,
          0.0032842967193573713,
          0.0031389787327498198,
          0.003006163751706481,
          0.0028854282572865486,
          0.002775507280603051,
          0.0026750783436000347,
          0.0025827172212302685,
          0.002497381065040827,
          0.0024180614855140448,
          0.00234392611309886,
          0.0022742105647921562,
          0.0022089567501097918,
          0.0021481411531567574,
          0.002091736299917102,
          0.002039363607764244,
          0.0019905806984752417,
          0.0019450061954557896,
          0.0019021208863705397,
          0.00186159648001194,
          0.0018231758149340749,
          0.0017866413109004498,
          0.0017519420944154263,
          0.0017189469654113054,
          0.001687486656010151,
          0.001657444634474814,
          0.0016287794569507241,
          0.0016013813437893987,
          0.0015751435421407223,
          0.001549951615743339,
          0.0015257014892995358,
          0.0015023474115878344,
          0.0014798303600400686,
          0.001458054524846375,
          0.0014369753189384937,
          0.001416612882167101,
          0.0013969428837299347,
          0.0013779043219983578,
          0.0013594356132671237,
          0.0013415053253993392,
          0.0013240998378023505,
          0.0013071896973997355,
          0.001290717045776546,
          0.0012746559223160148,
          0.001258987351320684,
          0.0012436764081940055,
          0.0012287106364965439,
          0.0012140895705670118,
          0.0011997767724096775,
          0.0011858086800202727,
          0.0011721622431650758,
          0.0011588024208322167,
          0.0011457238579168916,
          0.0011329231783747673,
          0.0011203923495486379,
          0.001108107273466885,
          0.0010960428044199944,
          0.0010841808980330825,
          0.0010725517058745027,
          0.0010611464967951179,
          0.0010499432682991028,
          0.0010389399249106646,
          0.0010281448485329747,
          0.0010175458155572414,
          0.0010071264114230871,
          0.0009968697559088469,
          0.000986766186542809,
          0.000976838287897408,
          0.0009670790750533342,
          0.0009574713767506182,
          0.0009480128646828234,
          0.0009387098252773285,
          0.0009295524214394391,
          0.0009205344831570983,
          0.000911643379367888,
          0.0009028696804307401,
          0.0008942388813011348,
          0.0008857441716827452,
          0.0008773694862611592,
          0.0008691144757904112,
          0.000860988802742213,
          0.0008529868791811168,
          0.000845093687530607,
          0.000837305560708046,
          0.0008296119049191475,
          0.000822033325675875,
          0.0008145676692947745,
          0.00080720434198156,
          0.0007999375811778009,
          0.0007927807746455073,
          0.000785722688306123,
          0.000778756570070982,
          0.0007718782871961594,
          0.0007650773040950298,
          0.000758373353164643,
          0.0007517627673223615,
          0.0007452411227859557,
          0.0007388003286905587,
          0.0007324504549615085,
          0.000726187601685524,
          0.0007200014661066234,
          0.0007138896617107093,
          0.0007078427588567138,
          0.0007018781616352499,
          0.0006959927850402892,
          0.0006901820888742805,
          0.0006844415329396725,
          0.0006787788588553667,
          0.0006731878966093063,
          0.0006676629418507218,
          0.0006621999200433493,
          0.0006567931268364191,
          0.0006514590932056308,
          0.0006461917073465884,
          0.0006409883499145508,
          0.0006358455284498632,
          0.000630769704002887,
          0.0006257587228901684,
          0.000620804843492806,
          0.000615903118159622,
          0.0006110514514148235,
          0.0006062620086595416,
          0.0006015316466800869,
          0.0005968562327325344,
          0.0005922324489802122,
          0.0005876714130863547,
          0.0005831631715409458,
          0.0005787057452835143,
          0.0005742929060943425,
          0.000569922907743603,
          0.0005656053544953465,
          0.0005613414687104523,
          0.000557125371415168,
          0.0005529549089260399,
          0.000548834097571671,
          0.0005447638686746359,
          0.0005407357239164412,
          0.0005367479170672596,
          0.0005327966646291316,
          0.0005288926186040044,
          0.0005250347894616425,
          0.0005212196265347302,
          0.0005174422403797507,
          0.0005137121770530939,
          0.0005100247217342257,
          0.0005063759163022041,
          0.0005027607548981905,
          0.0004991774912923574,
          0.0004956367774866521,
          0.0004921347717754543,
          0.0004886719980277121,
          0.0004852437705267221,
          0.00048185570631176233,
          0.0004785051860380918,
          0.0004751876404043287,
          0.00047190109035000205,
          0.000468641024781391,
          0.00046541940537281334,
          0.00046223122626543045,
          0.0004590777098201215,
          0.0004559553926810622,
          0.00045286849490366876,
          0.0004498138732742518,
          0.0004467903927434236,
          0.0004437931929714978,
          0.0004408188397064805,
          0.000437877926742658,
          0.0004349696100689471,
          0.00043208911665715277,
          0.00042923673754557967,
          0.0004264171584509313,
          0.00042362604290246964,
          0.0004208609461784363,
          0.00041811863775365055,
          0.0004153990594204515,
          0.0004127078573219478,
          0.000410045322496444,
          0.00040740828262642026,
          0.0004047964175697416,
          0.00040221205563284457,
          0.00039965510950423777,
          0.0003971206024289131,
          0.0003946075157728046,
          0.0003921138704754412,
          0.000389645661925897,
          0.0003872033266816288,
          0.00038478177157230675,
          0.00038238507113419473,
          0.00038001281791366637,
          0.0003776629746425897,
          0.0003753348719328642,
          0.00037302623968571424,
          0.0003707318683154881,
          0.0003684640978462994,
          0.0003662157978396863,
          0.00036398976226337254,
          0.00036178308073431253,
          0.0003595988964661956,
          0.00035743621992878616,
          0.0003552911221049726,
          0.0003531624097377062,
          0.0003510491515044123,
          0.00034895632416009903,
          0.00034688482992351055,
          0.00034483091440051794,
          0.00034279562532901764,
          0.000340779748512432,
          0.0003387836623005569,
          0.0003368027973920107,
          0.00033483767765574157,
          0.0003328874008730054,
          0.0003309535386506468,
          0.0003290392633061856,
          0.0003271415189374238,
          0.0003252580645494163,
          0.00032339521567337215,
          0.00032155014923773706,
          0.0003197167534381151,
          0.0003178993647452444,
          0.00031609315192326903,
          0.00031430349918082356,
          0.0003125302609987557,
          0.0003107719530817121,
          0.00030902816797606647,
          0.0003073005354963243,
          0.0003055886772926897,
          0.0003038910508621484,
          0.000302202592138201,
          0.0003005281905643642,
          0.000298868166282773,
          0.0002972227812279016,
          0.0002955911331810057,
          0.00029397165053524077,
          0.0002923674474004656,
          0.00029077869839966297,
          0.0002892013581003994,
          0.00028763432055711746,
          0.0002860776148736477,
          0.00028453487902879715,
          0.0002830049197655171,
          0.0002814882027450949,
          0.00027998225414194167,
          0.0002784915850497782,
          0.0002770129940472543,
          0.0002755463938228786,
          0.00027408829191699624,
          0.00027263900847174227,
          0.0002712029963731766,
          0.00026977804373018444,
          0.00026836697361432016,
          0.000266965595073998,
          0.00026557472301647067,
          0.00026419784990139306,
          0.00026282991166226566,
          0.00026147073367610574,
          0.000260121509199962,
          0.0002587822964414954,
          0.0002574539103079587,
          0.00025613655452616513,
          0.00025482839555479586,
          0.0002535332168918103,
          0.0002522474096622318,
          0.0002509711775928736,
          0.0002497025125194341,
          0.00024844237486831844,
          0.00024719134671613574,
          0.00024595099966973066,
          0.00024472008226439357,
          0.00024349801242351532,
          0.00024228819529525936,
          0.00024108534853439778,
          0.00023989318287931383,
          0.00023870653240010142,
          0.00023752832203172147,
          0.00023635885736439377,
          0.00023519707610830665,
          0.00023404670355375856,
          0.00023290314129553735,
          0.00023176969261839986,
          0.0002306456008227542,
          0.00022952807194087654,
          0.0002284167130710557,
          0.00022731312492396683,
          0.00022621717653237283,
          0.0002251297264592722,
          0.0002240507019450888,
          0.000222981019760482,
          0.00022191782773006707,
          0.00022086358512751758,
          0.00021981667669024318,
          0.000218775006942451,
          0.00021774003107566386,
          0.00021671106514986604,
          0.00021569266391452402,
          0.00021468033082783222,
          0.00021367458975873888,
          0.0002126771432813257,
          0.0002116867690347135,
          0.0002107047475874424,
          0.00020972710626665503,
          0.00020875393238384277,
          0.00020778887846972793,
          0.0002068315225187689,
          0.00020588030747603625,
          0.00020493488409556448,
          0.00020399958884809166,
          0.00020306839724071324,
          0.00020214510732330382,
          0.00020122523710597306,
          0.00020031226449646056,
          0.0001994050689972937,
          0.00019850354874506593,
          0.00019761022122111171,
          0.00019672133203130215,
          0.00019584133406169713,
          0.00019496624008752406,
          0.00019409632659517229,
          0.00019323098240420222,
          0.00019237142987549305,
          0.00019151595188304782,
          0.00019066780805587769,
          0.0001898268674267456,
          0.00018898963753599674,
          0.00018816010560840368,
          0.00018733479373622686,
          0.0001865155209088698,
          0.00018570059910416603,
          0.00018488845671527088,
          0.0001840836921473965,
          0.0001832839334383607,
          0.0001824907521950081,
          0.00018170078692492098,
          0.0001809180248528719,
          0.00018014013767242432,
          0.00017936722724698484,
          0.00017859786748886108,
          0.00017783221846912056,
          0.00017707253573462367,
          0.000176317582372576,
          0.0001755676930770278,
          0.00017482374096289277,
          0.0001740831066854298,
          0.0001733500830596313,
          0.0001726202026475221,
          0.00017189189384225756,
          0.0001711696822894737,
          0.00017045212734956294,
          0.00016973901074379683,
          0.0001690309145487845,
          0.00016832680557854474,
          0.0001676274259807542,
          0.00016693337238393724,
          0.0001662420982029289,
          0.00016555600450374186,
          0.00016487273387610912,
          0.00016419304301962256,
          0.00016351933300029486,
          0.00016285052697639912,
          0.00016218343807850033,
          0.00016152230091392994,
          0.0001608665188541636,
          0.0001602129195816815,
          0.00015956370043568313,
          0.00015891641669441015,
          0.0001582736149430275,
          0.00015763497503940016,
          0.00015700106450822204,
          0.00015636993339285254,
          0.00015574412827845663,
          0.00015512316895183176,
          0.00015450383943971246,
          0.0001538883661851287,
          0.00015327564324252307,
          0.0001526666892459616,
          0.0001520612568128854,
          0.0001514599280199036,
          0.0001508630666648969,
          0.0001502690720371902,
          0.00014967971947044134,
          0.00014909377205185592,
          0.00014851041487418115,
          0.0001479279453633353,
          0.00014735069999005646,
          0.00014677834406029433,
          0.00014620661386288702,
          0.0001456402096664533,
          0.00014507726882584393,
          0.00014451798051595688,
          0.0001439620100427419,
          0.0001434069126844406,
          0.0001428549294359982,
          0.00014230776287149638,
          0.00014176327385939658,
          0.00014122112770564854,
          0.00014068307064007968,
          0.00014014898624736816,
          0.00013961701188236475,
          0.0001390880352118984,
          0.00013856159057468176,
          0.00013803664478473365,
          0.00013751622464042157,
          0.00013699947157874703,
          0.0001364853815175593,
          0.00013597344513982534,
          0.00013546491391025484,
          0.00013495991879608482,
          0.00013445767399389297,
          0.00013395535643212497,
          0.00013345840852707624,
          0.00013296173710841686,
          0.0001324689801549539,
          0.0001319803559454158,
          0.00013149381265975535,
          0.00013101020886097103,
          0.00013052980648353696,
          0.0001300506992265582,
          0.00012957466242369264,
          0.00012910037185065448,
          0.0001286291517317295,
          0.00012816049274988472,
          0.00012769544264301658,
          0.00012723154213745147,
          0.0001267706393264234,
          0.00012631391291506588,
          0.00012585839431267232,
          0.00012540401075966656,
          0.00012495268310885876,
          0.00012450305803213269,
          0.00012405644520185888,
          0.00012361217522993684,
          0.00012317078653723001,
          0.0001227331958943978,
          0.000122297162306495,
          0.00012186289677629247,
          0.00012143031199229881,
          0.00012100039020879194,
          0.00012057104322593659,
          0.00012014565436402336,
          0.00011972244101343676,
          0.00011930118489544839,
          0.00011888358858413994,
          0.00011846734560094774,
          0.00011805156827904284,
          0.00011763966176658869,
          0.00011722950875991955,
          0.00011682001786539331,
          0.00011641505261650309,
          0.00011601150617934763,
          0.0001156078651547432,
          0.00011520938278408721,
          0.00011481294495752081,
          0.00011441731476224959,
          0.00011402335803722963,
          0.00011363098019501194,
          0.00011324074876029044,
          0.00011285315849818289,
          0.00011246812937315553,
          0.00011208317300770432,
          0.00011170162906637415,
          0.0001113230682676658,
          0.00011094583169324324,
          0.00011056895891670138,
          0.0001101940797525458,
          0.00010982179082930088,
          0.00010945076792268082,
          0.00010908194235526025,
          0.00010871562699321657,
          0.00010835030116140842,
          0.00010798784933285788,
          0.00010762727470137179,
          0.00010726747859735042,
          0.00010690993804018945,
          0.0001065531323547475,
          0.00010619939712341875,
          0.00010584726260276511,
          0.00010549685976002365,
          0.000105148101283703,
          0.00010480011405888945,
          0.00010445644147694111,
          0.00010411145922262222,
          0.00010376901627751067,
          0.00010342831956222653,
          0.00010308927448932081,
          0.0001027525941026397,
          0.00010241592826787382,
          0.00010208332241745666,
          0.00010175068018725142,
          0.00010142206883756444,
          0.0001010914784274064,
          0.0001007636237773113,
          0.00010043707879958674,
          0.00010011216363636777,
          0.00009978978778235614,
          0.0000994682777673006,
          0.00009914778638631105,
          0.00009882986341835931,
          0.00009851455979514867,
          0.00009819842671277002,
          0.00009788454917725176,
          0.00009757113002706319,
          0.00009726065763970837,
          0.00009695099288364872,
          0.0000966433944995515,
          0.00009633646550355479,
          0.00009603229409549385,
          0.0000957283700699918,
          0.00009542668703943491,
          0.00009512484393781051,
          0.00009482490713708103,
          0.00009452673111809418,
          0.00009423040319234133,
          0.00009393509390065446,
          0.00009364181460114196,
          0.00009334916830994189,
          0.00009305899584433064,
          0.0000927686269278638,
          0.00009247891284758225,
          0.00009219249477609992,
          0.00009190665878122672,
          0.00009162179776467383,
          0.00009133891580859199,
          0.00009105657954933122,
          0.00009077702998183668,
          0.00009049779328051955,
          0.00009021856385516003,
          0.00008994252129923552,
          0.00008966594032244757,
          0.00008939144754549488,
          0.00008911819895729423,
          0.00008884574344847351,
          0.00008857509237714112,
          0.00008830673323245719,
          0.00008803904347587377,
          0.00008777180482866243,
          0.00008750568667892367,
          0.00008724067447474226,
          0.00008697700104676187,
          0.0000867146736709401,
          0.00008645302295917645,
          0.000086194348114077,
          0.00008593669190304354,
          0.00008567737677367404,
          0.00008542118303012103,
          0.00008516534580849111,
          0.00008491146581945941,
          0.00008465755672659725,
          0.00008440639066975564,
          0.00008415467164013535,
          0.0000839052299852483,
          0.0000836564286146313,
          0.00008340929343830794,
          0.00008316228922922164,
          0.00008291725680464879,
          0.0000826726682134904,
          0.00008242733747465536,
          0.00008218600851250812,
          0.00008194510155590251,
          0.00008170521323336288,
          0.0000814660670584999,
          0.0000812283469713293,
          0.00008099075785139576,
          0.00008075494406512007,
          0.00008051852637436241,
          0.00008028510637814179,
          0.00008005173003766686,
          0.00007981985254446045,
          0.00007958876813063398,
          0.00007935972098493949,
          0.0000791306301834993,
          0.0000789026526035741,
          0.00007867461681598797,
          0.00007844873471185565,
          0.00007822277984814718,
          0.00007799967715982348,
          0.0000777757159085013,
          0.00007755433034617454,
          0.00007733221718808636,
          0.00007711263606324792,
          0.00007689172343816608,
          0.00007667347381357104,
          0.00007645547884749249,
          0.00007623796409461647,
          0.00007602317782584578,
          0.00007580833334941417,
          0.00007559406367363408,
          0.00007538139470852911,
          0.0000751689076423645,
          0.00007495813770219684,
          0.0000747469239286147,
          0.00007453673606505617,
          0.00007432940765284002,
          0.00007412122067762539,
          0.00007391339750029147,
          0.00007370692765107378,
          0.00007350283703999594,
          0.00007329812069656327,
          0.00007309435022762045,
          0.00007289140921784565,
          0.00007268793706316501,
          0.00007248748443089426,
          0.00007228793401736766,
          0.00007208852184703574,
          0.00007188934250734746,
          0.00007169215678004548,
          0.00007149457087507471,
          0.00007129891309887171,
          0.00007110235310392454,
          0.00007090784492902458,
          0.00007071351137710735,
          0.00007052087312331423,
          0.00007032808935036883,
          0.00007013633876340464,
          0.00006994667637627572,
          0.00006975589349167421,
          0.00006956636207178235,
          0.0000693779657012783,
          0.0000691903114784509,
          0.00006900267180753872,
          0.00006881551234982908,
          0.00006863150338176638,
          0.00006844630115665495,
          0.00006826176831964403,
          0.00006807931640651077,
          0.00006789638428017497,
          0.00006771401240257546,
          0.00006753300840500742,
          0.00006735307397320867,
          0.0000671722082188353,
          0.00006699362711515278,
          0.00006681502418359742,
          0.00006663887324975803,
          0.00006646040128543973,
          0.00006628474511671811,
          0.00006611046410398558,
          0.00006593309808522463,
          0.00006575923907803372,
          0.00006558666791534051,
          0.00006541279435623437,
          0.00006524119817186147,
          0.00006506993668153882,
          0.00006490003579529002,
          0.00006472897075582296,
          0.00006455900438595563,
          0.00006438972195610404,
          0.00006422298611141741,
          0.0000640547732473351,
          0.00006388621841324493,
          0.00006372118514264002,
          0.00006355615187203512,
          0.00006338992534438148,
          0.00006322610715869814,
          0.00006306219438556582,
          0.00006289935845416039,
          0.00006273452891036868,
          0.00006257386121433228,
          0.00006241184019017965,
          0.00006225262768566608,
          0.00006209143612068146,
          0.00006193338049342856,
          0.0000617737096035853,
          0.000061615755839739,
          0.00006145717634353787,
          0.0000613012962276116,
          0.00006114276038715616,
          0.0000609880116826389,
          0.000060832542658317834,
          0.00006067817594157532,
          0.00006052486423868686,
          0.00006037066486896947,
          0.00006021603985573165,
          0.000060064725403208286,
          0.00005991220677969977,
          0.00005976149986963719,
          0.00005961009082966484,
          0.00005946042438154109,
          0.000059311481891199946,
          0.00005916235750191845,
          0.00005901376425754279,
          0.000058864236052613705,
          0.00005871767643839121,
          0.00005857040741830133,
          0.00005842433893121779,
          0.00005827758286613971,
          0.00005813304233015515,
          0.00005798911661258899,
          0.00005784438326372765,
          0.000057700050092535093,
          0.000057557681429898366,
          0.000057415007177041844,
          0.00005727259485865943,
          0.00005713083010050468,
          0.00005698936729459092,
          0.00005685014548362233,
          0.00005670926839229651,
          0.0000565696791454684,
          0.00005643088661599904,
          0.00005629241422866471,
          0.0000561540546186734,
          0.000056016349844867364,
          0.00005587952182395384,
          0.000055742275435477495,
          0.0000556067461729981,
          0.000055471416999353096,
          0.00005533589865081012,
          0.00005520130071090534,
          0.00005506748129846528,
          0.00005493345815921202,
          0.00005480086474562995,
          0.00005466671791509725,
          0.000054535645176656544,
          0.00005440476525109261,
          0.00005427300129667856,
          0.0000541422632522881,
          0.00005401071030064486,
          0.000053881762141827494,
          0.000053751191444462165,
          0.000053623287385562435,
          0.00005349344064597972,
          0.00005336624235496856,
          0.000053238611144479364,
          0.000053111041779629886,
          0.00005298515679896809,
          0.00005285854422254488,
          0.000052732459153048694,
          0.00005260774923954159,
          0.00005248177330940962,
          0.000052356688684085384,
          0.00005223318294156343,
          0.000052109982789261267,
          0.000051985945901833475,
          0.000051864171837223694,
          0.00005174168472876772,
          0.00005161904118722305,
          0.00005149667413206771,
          0.000051376267947489396,
          0.000051255196012789384,
          0.000051135706598870456,
          0.00005101603164803237,
          0.000050896447646664456,
          0.000050776889111148193,
          0.00005065797449788079,
          0.000050540165830170736,
          0.00005042129851062782,
          0.00005030533793615177,
          0.00005018713272875175,
          0.00005006990613765083,
          0.00004995435301680118,
          0.00004983928010915406,
          0.000049723614210961387,
          0.000049607919208938256,
          0.00004949361755279824,
          0.000049379017582396045,
          0.00004926435212837532,
          0.00004915124372928403,
          0.00004903860462945886,
          0.000048925492592388764,
          0.00004881285349256359,
          0.000048700003389967605,
          0.00004859014370595105,
          0.0000484778756799642,
          0.000048365298425778747,
          0.00004825624273507856,
          0.00004814544445252977,
          0.00004803640331374481,
          0.00004792683830601163,
          0.00004781710231327452,
          0.00004770937084686011,
          0.00004760175943374634,
          0.00004749251820612699,
          0.00004738548523164354,
          0.000047277691919589415,
          0.00004717012416222133,
          0.00004706411709776148,
          0.00004695865209214389,
          0.000046852510422468185,
          0.00004674650699598715,
          0.00004664259176934138,
          0.00004653623545891605,
          0.00004643200009013526,
          0.00004632792479242198,
          0.00004622337291948497,
          0.00004612081102095544,
          0.00004601804539561272,
          0.00004591475226334296,
          0.000045811910240445286,
          0.000045709151891060174,
          0.00004560881279758178,
          0.000045507018512580544,
          0.00004540707959677093,
          0.000045304757804842666,
          0.00004520450602285564,
          0.00004510373037192039,
          0.00004500426075537689,
          0.00004490549326874316,
          0.00004480564166442491,
          0.00004470654312171973,
          0.000044608521420741454,
          0.00004451032873475924,
          0.00004441173223312944,
          0.00004431409979588352,
          0.00004421750418259762,
          0.00004411974805407226,
          0.000044024262024322525,
          0.00004392766277305782,
          0.0000438314164057374,
          0.000043735602957895026,
          0.00004364071719464846
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line(test_losses, log_y=True)\n",
    "# plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899
         ],
         "xaxis": "x",
         "y": [
          2.301990032196045,
          2.271068572998047,
          2.23994779586792,
          2.2092206478118896,
          2.176811695098877,
          2.1434266567230225,
          2.1076362133026123,
          2.070155382156372,
          2.0340754985809326,
          1.9980801343917847,
          1.9673292636871338,
          1.9375600814819336,
          1.9140328168869019,
          1.890297293663025,
          1.8703263998031616,
          1.8486641645431519,
          1.8285305500030518,
          1.813177466392517,
          1.7977007627487183,
          1.7850780487060547,
          1.7640475034713745,
          1.7470600605010986,
          1.7282990217208862,
          1.707759141921997,
          1.684143304824829,
          1.6615384817123413,
          1.6412895917892456,
          1.615416407585144,
          1.589840292930603,
          1.5554484128952026,
          1.5207288265228271,
          1.4847495555877686,
          1.447853684425354,
          1.4046601057052612,
          1.3644217252731323,
          1.321488857269287,
          1.273688554763794,
          1.2271744012832642,
          1.1740831136703491,
          1.1145362854003906,
          1.055804967880249,
          0.998585045337677,
          0.9344625473022461,
          0.8709927797317505,
          0.8093199729919434,
          0.745267391204834,
          0.6857129335403442,
          0.6265265345573425,
          0.5715181231498718,
          0.517713725566864,
          0.468394011259079,
          0.42351245880126953,
          0.37480154633522034,
          0.33606210350990295,
          0.291221559047699,
          0.2551015317440033,
          0.22010841965675354,
          0.18895862996578217,
          0.1602020114660263,
          0.13458159565925598,
          0.11567258834838867,
          0.0959727093577385,
          0.08155813813209534,
          0.06833237409591675,
          0.05873221904039383,
          0.04990093410015106,
          0.04259960353374481,
          0.036075372248888016,
          0.031222213059663773,
          0.02733251266181469,
          0.023842643946409225,
          0.021040376275777817,
          0.018808340653777122,
          0.016962168738245964,
          0.014978194609284401,
          0.013585934415459633,
          0.012280702590942383,
          0.011148259043693542,
          0.010222080163657665,
          0.009298953227698803,
          0.008500603027641773,
          0.007950401864945889,
          0.007358204107731581,
          0.006756930146366358,
          0.006286424584686756,
          0.005878004245460033,
          0.005444241221994162,
          0.005101026501506567,
          0.004754047840833664,
          0.004473147448152304,
          0.0042833187617361546,
          0.004071654286235571,
          0.0038156979717314243,
          0.0036514431703835726,
          0.0034920405596494675,
          0.0033131130039691925,
          0.0031851574312895536,
          0.003001949517056346,
          0.0028868354856967926,
          0.002803441369906068,
          0.0027193098794668913,
          0.0025895119179040194,
          0.002522281603887677,
          0.0024464253801852465,
          0.002365282503888011,
          0.0023117659147828817,
          0.0022096955217421055,
          0.0021499854046851397,
          0.0021094840485602617,
          0.002068719593808055,
          0.001994180493056774,
          0.001963801681995392,
          0.0019234666833654046,
          0.0018742409301921725,
          0.001853539957664907,
          0.0017879009246826172,
          0.0017546433955430984,
          0.0017314881552010775,
          0.0017078530509024858,
          0.0016591619933024049,
          0.0016421641921624541,
          0.0016184309497475624,
          0.001583910547196865,
          0.001575800939463079,
          0.0015281018568202853,
          0.0015064658364281058,
          0.0014901889953762293,
          0.001474240212701261,
          0.0014386108377948403,
          0.0014276693109422922,
          0.0014114391524344683,
          0.0013845361536368728,
          0.0013822632608935237,
          0.0013434424763545394,
          0.001328883576206863,
          0.0013156587956473231,
          0.001303959870710969,
          0.0012757342774420977,
          0.0012688622809946537,
          0.0012565639335662127,
          0.0012337457155808806,
          0.0012346971780061722,
          0.00120096979662776,
          0.0011905819410458207,
          0.0011792590375989676,
          0.0011705746874213219,
          0.0011463674018159509,
          0.0011415740009397268,
          0.001132023986428976,
          0.001112615573219955,
          0.0011146877659484744,
          0.001085318741388619,
          0.0010770575609058142,
          0.0010671202326193452,
          0.001060417271219194,
          0.0010392090771347284,
          0.0010360372252762318,
          0.0010282632429152727,
          0.0010110483272001147,
          0.0010136757045984268,
          0.000987728824838996,
          0.0009809726616367698,
          0.0009720977395772934,
          0.0009667680715210736,
          0.0009480101871304214,
          0.0009458890999667346,
          0.0009394578519277275,
          0.0009238924249075353,
          0.0009269340080209076,
          0.0009037085110321641,
          0.0008980572456493974,
          0.0008900508400984108,
          0.0008857574430294335,
          0.0008689725073054433,
          0.0008675504359416664,
          0.0008621698361821473,
          0.0008480476099066436,
          0.0008513244101777673,
          0.0008304223301820457,
          0.00082563137402758,
          0.0008183288737200201,
          0.0008147937478497624,
          0.0007997145876288414,
          0.0007988822762854397,
          0.0007942959782667458,
          0.000781369861215353,
          0.0007847623201087117,
          0.0007658340036869049,
          0.0007617971277795732,
          0.000755111628677696,
          0.0007521556690335274,
          0.000738547823857516,
          0.0007381337927654386,
          0.0007342075114138424,
          0.0007223595748655498,
          0.0007257675752043724,
          0.0007085629622451961,
          0.0007051440188661218,
          0.0006990195834077895,
          0.0006965243956074119,
          0.0006841927533969283,
          0.000684079306665808,
          0.0006807096651755273,
          0.0006698283250443637,
          0.0006731941248290241,
          0.000657488708384335,
          0.0006545586511492729,
          0.0006489493534900248,
          0.0006468213396146894,
          0.000635601463727653,
          0.0006357257370837033,
          0.0006328379386104643,
          0.0006228075362741947,
          0.0006261146045289934,
          0.0006117022130638361,
          0.000609199982136488,
          0.0006040645530447364,
          0.0006022504530847073,
          0.0005919968243688345,
          0.0005923061980865896,
          0.0005898429080843925,
          0.0005805619293823838,
          0.0005838011275045574,
          0.0005705235525965691,
          0.0005683886120095849,
          0.0005636804853565991,
          0.000562127330340445,
          0.0005527297034859657,
          0.0005531699862331152,
          0.0005510695627890527,
          0.0005424552364274859,
          0.0005456233047880232,
          0.0005333537119440734,
          0.0005315309972502291,
          0.000527199765201658,
          0.0005258712917566299,
          0.0005172318196855485,
          0.0005177754792384803,
          0.0005159850115887821,
          0.0005079704569652677,
          0.000511059770360589,
          0.0004996950156055391,
          0.0004981373785994947,
          0.000494147592689842,
          0.0004930117866024375,
          0.0004850531986448914,
          0.0004856671148445457,
          0.00048414740012958646,
          0.0004766701895277947,
          0.0004796783032361418,
          0.00046912222751416266,
          0.0004677890974562615,
          0.00046411162475124,
          0.00046313609345816076,
          0.00045578545541502535,
          0.0004564494302030653,
          0.00045515852980315685,
          0.00044817267917096615,
          0.00045109959319233894,
          0.00044126613647677004,
          0.0004401266050990671,
          0.00043673402979038656,
          0.00043589179404079914,
          0.0004290817305445671,
          0.0004297848790884018,
          0.0004286973853595555,
          0.0004221529816277325,
          0.00042499788105487823,
          0.0004158135561738163,
          0.00041483977111056447,
          0.0004117040953133255,
          0.000410973938414827,
          0.00040465276106260717,
          0.00040537910535931587,
          0.00040447135688737035,
          0.00039832876063883305,
          0.0004010929842479527,
          0.0003924966440536082,
          0.00039166491478681564,
          0.0003887642815243453,
          0.00038812780985608697,
          0.0003822518337983638,
          0.0003829921188298613,
          0.0003822420258074999,
          0.00037646389682777226,
          0.0003791486378759146,
          0.000371084752259776,
          0.0003703770926222205,
          0.0003676869091577828,
          0.0003671359736472368,
          0.00036165924393571913,
          0.00036240750341676176,
          0.00036179713788442314,
          0.0003563494828995317,
          0.0003589499683585018,
          0.00035137261147610843,
          0.0003507730725686997,
          0.0003482690663076937,
          0.0003477955178823322,
          0.00034268127637915313,
          0.0003434311074670404,
          0.00034294271608814597,
          0.00033779931254684925,
          0.0003403176961001009,
          0.0003331846965011209,
          0.000332680472638458,
          0.0003303417470306158,
          0.000329941714880988,
          0.0003251537855248898,
          0.0003259000077378005,
          0.00032551976619288325,
          0.0003206559631507844,
          0.0003230949805583805,
          0.00031636565108783543,
          0.00031594614847563207,
          0.00031375783146359026,
          0.00031342310830950737,
          0.00030893218354322016,
          0.0003096718282904476,
          0.00030937898554839194,
          0.0003047810459975153,
          0.00030713420710526407,
          0.0003007777559105307,
          0.00030043229344300926,
          0.0002983839367516339,
          0.0002981042198371142,
          0.00029388206894509494,
          0.00029461688245646656,
          0.0002944006701000035,
          0.00029004650423303246,
          0.0002923195715993643,
          0.00028630546876229346,
          0.000286028312984854,
          0.00028410652885213494,
          0.0002838734944816679,
          0.0002798969217110425,
          0.0002806284464895725,
          0.0002804772520903498,
          0.0002763477386906743,
          0.00027854222571477294,
          0.00027284384123049676,
          0.0002726298407651484,
          0.00027082403539679945,
          0.0002706336963456124,
          0.00026688427897170186,
          0.00026760660693980753,
          0.0002675143477972597,
          0.00026358963805250823,
          0.00026571014313958585,
          0.0002603039611130953,
          0.0002601470332592726,
          0.00025844815536402166,
          0.00025829518563114107,
          0.00025474984431639314,
          0.0002554673992563039,
          0.00025542356888763607,
          0.0002516921085771173,
          0.0002537420659791678,
          0.00024860582198016346,
          0.00024849988403730094,
          0.00024689818383194506,
          0.0002467793528921902,
          0.00024342343385796994,
          0.00024413126811850816,
          0.0002441310789436102,
          0.00024057865084614605,
          0.00024255788594018668,
          0.00023767542734276503,
          0.00023761378542985767,
          0.00023610061907675117,
          0.00023601397697348148,
          0.00023283051268663257,
          0.0002335268072783947,
          0.0002335682074772194,
          0.00023018075444269925,
          0.00023209214850794524,
          0.00022744557645637542,
          0.00022742045985069126,
          0.0002259925240650773,
          0.00022593094035983086,
          0.000222908376599662,
          0.00022359447029884905,
          0.00022366979101207107,
          0.00022043616627343,
          0.00022228500165510923,
          0.00021785787248518318,
          0.00021786575962323695,
          0.00021651785937137902,
          0.00021647852554451674,
          0.0002136028342647478,
          0.00021427958563435823,
          0.00021438280236907303,
          0.00021129508968442678,
          0.00021308135183062404,
          0.00020885842968709767,
          0.00020889700681436807,
          0.00020762217172887176,
          0.00020760312327183783,
          0.00020486643188633025,
          0.00020553162903524935,
          0.00020566035527735949,
          0.00020270806271582842,
          0.00020443453104235232,
          0.00020040613890159875,
          0.00020046760619152337,
          0.0001992624020203948,
          0.00019926088862121105,
          0.0001966545096365735,
          0.0001973066246137023,
          0.00019745994359254837,
          0.00019463167700450867,
          0.00019630312453955412,
          0.00019245412840973586,
          0.00019253518257755786,
          0.00019139597134198993,
          0.00019141126540489495,
          0.00018892389198299497,
          0.00018956430722028017,
          0.00018973635451402515,
          0.00018702638044487685,
          0.00018864368030335754,
          0.000184961871127598,
          0.00018506291962694377,
          0.0001839831384131685,
          0.00018401276611257344,
          0.00018163624918088317,
          0.00018226401880383492,
          0.0001824552018661052,
          0.0001798549055820331,
          0.00018141807231586426,
          0.0001778965670382604,
          0.00017801379726734012,
          0.00017698899318929762,
          0.00017703097546473145,
          0.00017475987260695547,
          0.0001753754186211154,
          0.00017558271065354347,
          0.00017308622773271054,
          0.00017459677474107593,
          0.00017122688586823642,
          0.00017135788220912218,
          0.00017038661462720484,
          0.0001704374299151823,
          0.0001682633883319795,
          0.00016886909725144506,
          0.00016908835095819086,
          0.00016668780881445855,
          0.0001681517605902627,
          0.00016492295253556222,
          0.0001650651392992586,
          0.00016414219862781465,
          0.00016420264728367329,
          0.00016212051559705287,
          0.00016271538333967328,
          0.0001629468606552109,
          0.00016063857765402645,
          0.00016205667634494603,
          0.0001589602034073323,
          0.00015911283844616264,
          0.0001582356489961967,
          0.0001583019911777228,
          0.0001563067053211853,
          0.0001568914158269763,
          0.0001571332395542413,
          0.0001549104053992778,
          0.00015628435357939452,
          0.00015331286704167724,
          0.00015347350563388318,
          0.00015263927343767136,
          0.00015271086886059493,
          0.00015079951845109463,
          0.0001513726165285334,
          0.00015162226918619126,
          0.0001494823954999447,
          0.0001508150016888976,
          0.0001479599013691768,
          0.00014812688459642231,
          0.00014733619173057377,
          0.0001474116725148633,
          0.0001455760357202962,
          0.00014613995153922588,
          0.00014639760775025934,
          0.000144334597280249,
          0.00014562490105163306,
          0.00014288185047917068,
          0.0001430543779861182,
          0.00014230083615984768,
          0.00014238145377021283,
          0.00014061776164453477,
          0.00014117239334154874,
          0.0001414365106029436,
          0.0001394440623698756,
          0.00014069740427657962,
          0.00013805933122057468,
          0.000138235351187177,
          0.00013751850929111242,
          0.0001376036088913679,
          0.00013590687012765557,
          0.0001364522468065843,
          0.00013672148634213954,
          0.00013479792687576264,
          0.00013601405953522772,
          0.00013347754429560155,
          0.00013365519407670945,
          0.00013297161785885692,
          0.0001330589147983119,
          0.0001314260734943673,
          0.0001319626608164981,
          0.00013223545101936907,
          0.0001303771568927914,
          0.0001315593544859439,
          0.000129115825984627,
          0.00012929728836752474,
          0.0001286463375436142,
          0.00012873606465291232,
          0.00012716064520645887,
          0.00012768949090968817,
          0.00012796740338671952,
          0.00012617082393262535,
          0.00012731879542116076,
          0.000124965314171277,
          0.00012514674745034426,
          0.00012452545342966914,
          0.00012461686856113374,
          0.00012309869634918869,
          0.00012361937842797488,
          0.00012389985204208642,
          0.00012216343020554632,
          0.00012327880540397018,
          0.0001210101690958254,
          0.00012119189341319725,
          0.00012059907021466643,
          0.00012069315562257543,
          0.00011922812700504437,
          0.00011973911750828847,
          0.00012002227595075965,
          0.00011833906319225207,
          0.00011942535638809204,
          0.00011723684292519465,
          0.00011741912749130279,
          0.00011685430217767134,
          0.00011694955173879862,
          0.0001155335339717567,
          0.00011603676102822646,
          0.00011632197856670246,
          0.00011469283344922587,
          0.0001157494043582119,
          0.00011363637167960405,
          0.00011382007505744696,
          0.00011327788524795324,
          0.0001133746700361371,
          0.0001120075976359658,
          0.0001125007911468856,
          0.00011278777674306184,
          0.00011121023271698505,
          0.0001122385510825552,
          0.00011019755766028538,
          0.00011038122465834022,
          0.00010986238339683041,
          0.00010995881166309118,
          0.00010863868374144658,
          0.00010912407014984637,
          0.00010940999345621094,
          0.00010788103099912405,
          0.00010888336692005396,
          0.00010691144416341558,
          0.00010709406342357397,
          0.00010659772669896483,
          0.0001066961485776119,
          0.00010542001109570265,
          0.00010589690646156669,
          0.00010618211672408506,
          0.00010469984408700839,
          0.00010567633580649272,
          0.0001037681577145122,
          0.00010395183926448226,
          0.00010347592615289614,
          0.00010357455175835639,
          0.00010233847569907084,
          0.00010280816786689684,
          0.00010309489880455658,
          0.00010165637650061399,
          0.00010260617273161188,
          0.00010076124453917146,
          0.00010094331082655117,
          0.00010048670083051547,
          0.0001005862868623808,
          0.00009938951552612707,
          0.00009985054202843457,
          0.00010013727296609432,
          0.00009873984527075663,
          0.00009966702054953203,
          0.00009787989984033629,
          0.00009806102752918378,
          0.00009762388799572363,
          0.00009772311750566587,
          0.00009656447218731046,
          0.00009701723320176825,
          0.0000973042770056054,
          0.00009594548464519903,
          0.00009685056284070015,
          0.00009511932148598135,
          0.0000952991031226702,
          0.0000948795277508907,
          0.00009497906285105273,
          0.00009385561861563474,
          0.00009430235513718799,
          0.0000945873252931051,
          0.0000932676630327478,
          0.0000941497492021881,
          0.00009247234993381426,
          0.00009265122207580134,
          0.00009224807581631467,
          0.00009234821482095867,
          0.00009125890210270882,
          0.00009169741679215804,
          0.00009198264160659164,
          0.00009069905354408547,
          0.00009155816223938018,
          0.00008993388473754749,
          0.0000901096427696757,
          0.00008972325304057449,
          0.00008982264262158424,
          0.000088765453256201,
          0.00008919710671762004,
          0.00008948146569309756,
          0.00008823328244034201,
          0.00008907149458536878,
          0.00008749649714445695,
          0.00008767056715441868,
          0.00008729867840884253,
          0.00008739794429857284,
          0.00008637202699901536,
          0.00008679823076818138,
          0.00008708085806574672,
          0.00008586419426137581,
          0.00008668270311318338,
          0.00008515497029293329,
          0.00008532760693924502,
          0.00008497003000229597,
          0.00008506996528012678,
          0.00008407289715250954,
          0.0000844916285132058,
          0.00008477184746880084,
          0.00008358899503946304,
          0.00008438771328656003,
          0.00008290573896374553,
          0.00008307619282277301,
          0.0000827311523607932,
          0.00008282998169306666,
          0.00008186345075955614,
          0.00008227559010265395,
          0.00008255357533926144,
          0.00008140203863149509,
          0.00008218085713451728,
          0.00008074261131696403,
          0.00008090945630101487,
          0.00008057924424065277,
          0.00008067831367952749,
          0.00007973733590915799,
          0.00008014315244508907,
          0.00008042050467338413,
          0.0000792976570664905,
          0.00008005924610188231,
          0.00007866100349929184,
          0.00007882807403802872,
          0.00007850825932109728,
          0.00007860839104978368,
          0.00007769309013383463,
          0.00007809328963048756,
          0.00007836746226530522,
          0.00007727299089310691,
          0.00007801698666298762,
          0.00007665866723982617,
          0.00007682364230277017,
          0.00007651581836398691,
          0.0000766144585213624,
          0.00007572530739707872,
          0.00007611827459186316,
          0.00007639094110345468,
          0.00007532360177719966,
          0.00007605203427374363,
          0.00007473136065527797,
          0.00007489421841455624,
          0.00007459901098627597,
          0.00007469661068171263,
          0.00007383071351796389,
          0.00007421689224429429,
          0.00007448897667927667,
          0.00007344682671828195,
          0.00007415891013806686,
          0.00007287473272299394,
          0.00007303550228243694,
          0.00007274961535586044,
          0.00007284804451046512,
          0.00007200597610790282,
          0.0000723857301636599,
          0.00007265424210345373,
          0.00007163833652157336,
          0.00007233552605612203,
          0.0000710843742126599,
          0.00007124624971766025,
          0.00007096906483639032,
          0.00007106604607542977,
          0.00007024521619314328,
          0.00007062072836561128,
          0.00007088622805895284,
          0.0000698945441399701,
          0.00007057568291202188,
          0.00006935915007488802,
          0.00006951892282813787,
          0.00006925214984221384,
          0.00006934661359991878,
          0.00006854895036667585,
          0.00006891774683026597,
          0.0000691800523782149,
          0.00006821381248300895,
          0.00006887941708555445,
          0.00006769465107936412,
          0.00006785267032682896,
          0.00006759525422239676,
          0.00006768861203454435,
          0.00006691108137601987,
          0.0000672740425216034,
          0.00006753613706678152,
          0.00006659032078459859,
          0.00006724255945300683,
          0.00006608951662201434,
          0.00006624406523769721,
          0.00006599527841899544,
          0.00006608924741158262,
          0.00006533063424285501,
          0.00006568824028363451,
          0.00006594687147298828,
          0.00006502569158328697,
          0.00006566262163687497,
          0.00006453745299950242,
          0.00006469259096775204,
          0.000064452899096068,
          0.0000645443651592359,
          0.0000638034543953836,
          0.00006415706593543291,
          0.00006441424193326384,
          0.0000635113101452589,
          0.00006413574010366574,
          0.00006304005364654586,
          0.00006319398380583152,
          0.00006295914499787614,
          0.00006305065471678972,
          0.00006232900341274217,
          0.00006267743447097018,
          0.00006293020851444453,
          0.00006205063255038112,
          0.00006266161653911695,
          0.00006159243639558554,
          0.00006174383452162147,
          0.00006152060814201832,
          0.00006160731572890654,
          0.000060904480051249266,
          0.00006124709034338593,
          0.000061498794821091,
          0.00006063778346288018,
          0.00006123663479229435,
          0.00006019217107677832,
          0.000060344140365486965,
          0.000060125708841951564,
          0.00006021382432663813,
          0.0000595268102188129,
          0.000059864858485525474,
          0.000060114263760624453,
          0.000059271755162626505,
          0.000059858051827177405,
          0.000058838904806179926,
          0.00005898890594835393,
          0.00005877863077330403,
          0.00005886489088879898,
          0.000058194105804432184,
          0.000058528130466584116,
          0.00005877532021258958,
          0.00005795078686787747,
          0.00005852470712852664,
          0.000057531229685992,
          0.000057680044847074896,
          0.000057475710491416976,
          0.00005756008977186866,
          0.00005690605757990852,
          0.00005723576759919524,
          0.00005747838440584019,
          0.000056671906349947676,
          0.00005723509821109474,
          0.00005626518031931482,
          0.000056412332924082875,
          0.000056214277719845995,
          0.00005629828592645936,
          0.00005565887113334611,
          0.00005598340794676915,
          0.00005622449316433631,
          0.00005543442603084259,
          0.00005598719508270733,
          0.000055039996368577704,
          0.00005518439138540998,
          0.00005499413964571431,
          0.00005507491368916817,
          0.000054452371841762215,
          0.000054772128351032734,
          0.00005500989573192783,
          0.00005423790935310535,
          0.00005477888771565631,
          0.00005385379699873738,
          0.00005399627843871713,
          0.000053811760153621435,
          0.000053891355491941795,
          0.000053282052249414846,
          0.000053598032536683604,
          0.00005383332245401107,
          0.0000530778088432271,
          0.00005360872091841884,
          0.000052702973334817216,
          0.000052846404287265614,
          0.000052666349802166224,
          0.00005274572686175816,
          0.000052149513066979125,
          0.00005246133150649257,
          0.000052693369070766494,
          0.000051953607908217236,
          0.00005247439185041003,
          0.00005158861677045934,
          0.00005172856253921054,
          0.00005155688995728269,
          0.00005163446985534392,
          0.00005105164018459618,
          0.00005135930405231193,
          0.00005158887506695464,
          0.000050863975047832355,
          0.000051374907343415543,
          0.00005050955951446667,
          0.00005064783545094542,
          0.000050481397920520976,
          0.00005055640576756559,
          0.00004998631993657909,
          0.000050290487706661224,
          0.00005051810512668453,
          0.0000498075460200198,
          0.00005030847023590468,
          0.00004946239278069697,
          0.00004959994839737192,
          0.00004943623207509518,
          0.00004951289156451821,
          0.00004895469101029448,
          0.00004925378743791953,
          0.000049478327127872035,
          0.0000487815668748226,
          0.000049275255150860175,
          0.00004844596332986839,
          0.00004858102693106048,
          0.000048424219130538404,
          0.000048499197873752564,
          0.000047953602916095406,
          0.000048247802624246106,
          0.00004846909359912388,
          0.00004778803850058466,
          0.00004827157317777164,
          0.000047460209316341206,
          0.00004759611329063773,
          0.00004744193938677199,
          0.00004751505912281573,
          0.00004698144766734913,
          0.00004727340638055466,
          0.00004749152503791265,
          0.00004682282451540232,
          0.00004729807915282436,
          0.000046502846089424565,
          0.00004663770232582465,
          0.00004648849062505178,
          0.00004656014425563626,
          0.00004603829074767418,
          0.000046326244046213105,
          0.000046541401388822123,
          0.00004588545925798826,
          0.00004635014192899689,
          0.00004557508509606123,
          0.00004570792953018099,
          0.00004556403655442409,
          0.000045633543777512386,
          0.0000451212799816858,
          0.00004540538429864682,
          0.000045619421143783256,
          0.000044976233766647056,
          0.000045432938350131735,
          0.00004467231337912381,
          0.000044805212382925674,
          0.00004466392783797346,
          0.00004473371882340871,
          0.00004423230348038487,
          0.00004451301720109768,
          0.000044723052269546315,
          0.00004409309622133151,
          0.0000445414443674963,
          0.00004379630263429135,
          0.0000439273972006049
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line(train_losses, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): Identity()\n",
       "      (ln2): Identity()\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0.04035932,
           0.029771073,
           -0.079525284,
           -0.07045816,
           0.0065640667,
           -0.051558655,
           0.13125227,
           -0.050364554,
           0.041092288,
           0.12192161,
           -0.110844955,
           -0.043121476,
           -0.007882758,
           0.059765283,
           -0.13408084,
           0.19040377,
           0.11259651,
           -0.0535696,
           0.015645344,
           0.1250925,
           -0.1912154,
           0.07725189,
           0.1613439,
           0.11950842,
           -0.022205483,
           -0.19095875,
           -0.0011097513,
           -0.035015263,
           0.06270127,
           -0.086312845,
           0.18440898,
           0.2287768,
           -0.03026636,
           0.020426994,
           0.1423712,
           0.010953128,
           0.091900416,
           0.0725415,
           -0.10935162,
           -0.038411397,
           -0.03639492,
           -0.040714446,
           0.05679053,
           -0.17807752,
           -0.12870769,
           0.12336261,
           -0.12209892,
           -0.06966297,
           0.20463257,
           -0.060276493,
           0.18591124,
           -0.09026923,
           -0.061819077,
           0.22671603,
           0.2882163,
           -0.17294598,
           0.17324777,
           -0.15289728,
           0.040829286,
           -0.0704618,
           0.08270246,
           0.1433517,
           0.18575166,
           -0.09204035,
           -0.059671715,
           0.083811216,
           -0.02460198,
           -0.03347507,
           -0.06412723,
           0.08578056,
           -0.059936643,
           -0.16090801,
           0.22262797,
           -0.02616868,
           0.0802318,
           0.08422481,
           -0.1474909,
           0.10960183,
           0.018499877,
           -0.007111309,
           0.25180176,
           -0.10654485,
           0.040309228,
           0.0064431685,
           -0.1265317,
           -0.14316776,
           0.10663606,
           0.028643621,
           0.057494473,
           -0.022716552,
           0.083422974,
           -0.09757332,
           -0.0815014,
           -0.06312173,
           -0.14378928,
           -0.11575544,
           -0.15332954,
           -0.16301842,
           0.00049413246,
           -0.0057529574,
           0.07724298,
           -0.09429253,
           -0.07662827,
           0.0685546,
           -0.09286829,
           0.042021327,
           0.051810827,
           0.16904901,
           0.008364196,
           0.26926264,
           -0.124230094,
           -0.21966955,
           -0.026307615,
           -0.008908639,
           0.13963078,
           -0.2088564,
           -0.02223574,
           0.030594213,
           -0.20513587,
           0.06787207,
           0.17130716,
           0.01534965,
           0.08286861,
           -0.03891293,
           -0.112555,
           0.0819529,
           -0.051276077,
           -0.19009742
          ],
          [
           -0.23355646,
           0.19199763,
           0.17809612,
           -0.040530168,
           0.19230643,
           0.16813324,
           0.04853789,
           -0.17449045,
           -0.002780209,
           0.013793087,
           0.13699707,
           0.0757027,
           0.091417566,
           -0.06820377,
           0.14041942,
           0.13777144,
           0.23200858,
           0.1520176,
           -0.17057031,
           -0.081867635,
           0.084153555,
           -0.04167982,
           -0.105157174,
           0.1770451,
           -0.10334025,
           -0.014101562,
           0.17065452,
           -0.052063394,
           -0.12851201,
           -0.06336769,
           0.20685336,
           -0.027565919,
           -0.1147138,
           -0.064569324,
           0.024703527,
           0.24363667,
           -0.28668126,
           0.07628202,
           -0.05963618,
           -0.066302836,
           0.019334577,
           0.24436484,
           -0.03265386,
           0.017098643,
           -0.17512326,
           0.06310371,
           -0.22616899,
           -0.19912234,
           -0.07570129,
           -0.11495473,
           0.025886439,
           0.0064548184,
           0.09622079,
           0.14202817,
           -0.21804644,
           0.07392898,
           0.07502623,
           0.16246809,
           0.08907685,
           -0.1419149,
           -0.08169228,
           0.19170016,
           0.045033626,
           0.15846303,
           0.08508395,
           -0.07695082,
           0.21226819,
           0.079035826,
           0.15604219,
           -0.04917347,
           -0.044491164,
           0.026564658,
           -0.00017557232,
           -0.19061996,
           0.14197206,
           0.0024120342,
           -0.0989258,
           -0.10990998,
           0.00023395353,
           0.06765052,
           0.015246185,
           -0.114333875,
           -0.015217221,
           -0.054447573,
           0.024202729,
           0.114125796,
           0.14122061,
           0.023378182,
           0.11463316,
           0.14416066,
           -0.017057313,
           -0.007308364,
           -0.05576322,
           0.06361964,
           -0.08431767,
           -0.024310037,
           -0.10446728,
           0.007065474,
           0.09330964,
           0.026948621,
           -0.042780418,
           0.12209373,
           0.032311633,
           0.014189596,
           0.12092737,
           -0.17665745,
           0.121302046,
           -0.21618246,
           -0.043044694,
           -0.054973662,
           -0.062828965,
           -0.024495855,
           0.15093309,
           -0.029860131,
           -0.057773814,
           -0.10540944,
           0.0456305,
           -0.08192823,
           -0.053109586,
           -0.14604144,
           -0.076976396,
           -0.048852354,
           -0.04021635,
           -0.17225756,
           0.088330746,
           -0.20826447,
           -0.07268109,
           0.15509988
          ],
          [
           0.123936705,
           -0.14651202,
           -0.13929696,
           0.21733965,
           0.04466528,
           0.032986347,
           0.14956246,
           -0.039496083,
           -0.2191877,
           -0.014240711,
           -0.13646618,
           -0.03887527,
           -0.15942986,
           0.0066174986,
           -0.08477276,
           -0.17094134,
           0.009452958,
           0.1332132,
           -0.09860195,
           0.033923145,
           0.13656753,
           0.008862829,
           -0.14568126,
           0.121037684,
           -0.11870613,
           -0.17432815,
           -0.27810544,
           -0.2965135,
           0.06562662,
           -0.058492027,
           -0.053566694,
           -0.27084628,
           0.07659307,
           0.056094248,
           -0.11351083,
           0.10227841,
           -0.057638258,
           -0.036003858,
           -0.18679136,
           -0.10132174,
           -0.0016828753,
           -0.13979587,
           0.04341199,
           0.12087468,
           0.092047535,
           -0.16869074,
           0.023339422,
           0.06176125,
           0.053760998,
           -0.19607557,
           -0.08551397,
           0.06759404,
           0.13202547,
           0.04757189,
           0.15190987,
           -0.15583885,
           -0.16052008,
           0.17619525,
           -0.15061367,
           0.083680905,
           -0.16001473,
           0.10975966,
           -0.12411436,
           0.15859582,
           0.18293779,
           0.0542399,
           -0.046250667,
           0.2761922,
           0.0005946784,
           0.07704489,
           0.034059383,
           -0.07931163,
           -0.16066071,
           0.032655254,
           0.029898284,
           -0.17173457,
           0.054889943,
           0.04648394,
           0.016773624,
           -0.0046259505,
           -0.043190826,
           -0.13495046,
           0.11678403,
           0.09107427,
           -0.10237034,
           -0.021588806,
           -0.240893,
           0.01484084,
           -0.035223324,
           -0.06533237,
           0.053277873,
           -0.16217697,
           0.08830675,
           -0.1146702,
           0.10710325,
           -0.09547927,
           -0.057875086,
           0.15189618,
           0.08711202,
           -0.0336797,
           -0.23659326,
           0.0029262556,
           -0.1591377,
           0.12435409,
           -0.08434553,
           0.17046148,
           -0.05025883,
           0.031110879,
           0.19509207,
           0.103396066,
           -0.17601693,
           0.124704465,
           -0.051854257,
           -0.21172886,
           0.053311016,
           0.13098027,
           -0.13829012,
           0.048150074,
           -0.07612386,
           -0.08519379,
           0.04513948,
           0.043318454,
           -0.030216912,
           -0.12061877,
           0.15759227,
           0.10687873,
           -0.03953655,
           0.015305962
          ],
          [
           0.026662542,
           -0.016442083,
           0.075357355,
           -0.10743816,
           0.12533508,
           0.18835369,
           0.020386146,
           0.03646957,
           -0.02240418,
           -0.12232472,
           0.100579135,
           -0.116329275,
           0.19907722,
           -0.2204219,
           0.097011894,
           -0.041346665,
           -0.060033873,
           0.063437596,
           -0.1458796,
           -0.07597349,
           -0.06887778,
           -0.11423962,
           -0.04329683,
           -0.15141757,
           0.12381046,
           -0.11209513,
           -0.18283994,
           0.10222307,
           -0.067091994,
           -0.0714593,
           0.051295172,
           0.045458734,
           0.014978394,
           0.14329638,
           -0.23270398,
           -0.14086767,
           0.09305007,
           0.30910134,
           -0.34284014,
           -0.13339566,
           0.063567385,
           -0.13070762,
           -0.032312788,
           -0.025542838,
           0.1172268,
           -0.12147809,
           0.10903813,
           0.13067856,
           -0.19203699,
           0.13732271,
           0.021404147,
           -0.24040376,
           -0.120175555,
           0.22267643,
           0.1690126,
           -0.17780991,
           0.21294679,
           -0.015410082,
           -0.023075318,
           -0.016519526,
           0.19336924,
           0.011317213,
           0.0016412173,
           0.08380015,
           0.08385969,
           -0.050993443,
           0.13515489,
           -0.1075547,
           -0.17955364,
           -0.07713377,
           -0.06716094,
           0.028594002,
           -0.18934913,
           -0.011572085,
           -0.019409114,
           -0.09605767,
           -0.09415077,
           -0.031791992,
           0.041254483,
           0.09373557,
           -0.034297217,
           -0.19848746,
           -0.22846122,
           -0.13278997,
           0.035092063,
           0.101215765,
           -0.07883207,
           0.0356263,
           -0.016315056,
           -0.008255776,
           -0.014543618,
           -0.14499468,
           0.15938987,
           -0.15437143,
           0.18981513,
           0.061289538,
           0.1037944,
           -0.015031704,
           -0.14034182,
           0.051490262,
           0.13818063,
           -0.013346264,
           -0.008546902,
           -0.05320051,
           0.09105729,
           -0.04600974,
           0.012639496,
           -0.115118496,
           0.015714541,
           -0.016248126,
           -0.060955614,
           0.06731257,
           -0.026652884,
           0.086382605,
           0.0650741,
           -0.20669912,
           0.2177385,
           -0.07070919,
           0.039713234,
           0.09060349,
           -0.029058017,
           -0.019041548,
           0.118636005,
           -0.003386647,
           0.18028668,
           -0.021687375,
           0.037455346,
           -0.05806005
          ],
          [
           -0.118118696,
           0.17601289,
           -0.047040533,
           -0.018669166,
           0.194254,
           -0.05078302,
           -0.1939607,
           0.00054657203,
           0.07805247,
           0.14693162,
           -0.22514415,
           -0.17553857,
           -0.060080275,
           -0.21280466,
           -0.07184071,
           -0.1257282,
           0.11874959,
           -0.16792947,
           -0.0026701798,
           -0.17023563,
           0.20949171,
           -0.080277,
           -0.02863167,
           -0.09393588,
           0.17899267,
           -0.08497357,
           -0.17449388,
           -0.05593947,
           -0.0077071064,
           0.16261554,
           0.07166474,
           0.03606372,
           0.043594528,
           0.03558222,
           0.0065833484,
           0.08298276,
           0.069766104,
           -0.018854272,
           0.051706374,
           0.19175021,
           -0.073214926,
           -0.07790919,
           0.07322638,
           0.0780554,
           -0.18814224,
           -0.058044024,
           0.043104485,
           0.102442876,
           0.047731377,
           0.08424302,
           0.073095694,
           0.03399025,
           -0.03373372,
           -0.003975253,
           -0.15437447,
           0.17753531,
           0.12641318,
           0.09279505,
           -0.036770966,
           -0.0072716787,
           -0.08717299,
           0.11887895,
           0.057571474,
           -0.06528482,
           -0.05231056,
           -0.008861495,
           -0.111565895,
           0.03987319,
           0.07318854,
           -0.25373375,
           0.1108978,
           0.1464126,
           -0.07759753,
           -0.12383547,
           -0.11286403,
           0.09723036,
           0.05007154,
           -0.13977648,
           0.09495917,
           -0.03249091,
           -0.07427544,
           0.024425097,
           0.08239316,
           0.20274045,
           -0.22034895,
           -0.1984288,
           -0.10946367,
           0.072830096,
           0.14863618,
           0.106270224,
           0.010857021,
           0.123815276,
           -0.19970548,
           0.10803427,
           0.053083908,
           -0.057638675,
           0.12966582,
           -0.23064709,
           0.063951075,
           -0.13473397,
           0.043858983,
           -0.09421753,
           0.23868696,
           -0.13240314,
           0.27606025,
           0.15054186,
           0.00041169603,
           -0.0064813113,
           0.08019017,
           -0.067134775,
           -0.23837338,
           -0.039422136,
           -0.024982667,
           0.0062515787,
           0.071453065,
           0.013229501,
           -0.016182495,
           0.2756634,
           -0.1754568,
           -0.06832187,
           -0.07280649,
           -0.1868665,
           -0.04184,
           0.089448445,
           -0.14958586,
           0.11730694,
           0.1020776,
           0.06394516
          ],
          [
           -0.11694165,
           0.017538985,
           0.15915288,
           -0.066529825,
           -0.0883872,
           0.022126373,
           -0.21237887,
           0.096144795,
           -0.0042218273,
           -0.27941477,
           -0.16018376,
           -0.07903301,
           -0.05802427,
           -0.050276272,
           0.089888655,
           0.07477846,
           -0.01605637,
           -0.2018622,
           0.17625129,
           0.18596213,
           0.031184234,
           -0.12290174,
           -0.14386143,
           0.074009955,
           -0.05949748,
           0.3357862,
           -0.0009542804,
           0.045471832,
           -0.176762,
           0.0524384,
           -0.11439372,
           0.08206302,
           -0.10312307,
           -0.118157946,
           0.022293149,
           0.083576776,
           -0.025262484,
           -0.1512379,
           0.10175175,
           -0.13693562,
           -0.17037666,
           -0.2200582,
           0.11485734,
           -0.074857205,
           0.13623796,
           -0.051319107,
           -0.016404748,
           -0.14628188,
           0.09230454,
           0.015788278,
           0.03890036,
           -0.16091734,
           0.076787345,
           -0.037671518,
           -0.18236545,
           0.13300148,
           0.062887385,
           0.0050906097,
           -0.0685441,
           0.024226522,
           0.039988153,
           -0.21350911,
           -0.14318411,
           -0.074548095,
           0.08501286,
           0.08853285,
           0.027816389,
           -0.12796736,
           0.020410167,
           -0.049812946,
           0.10760672,
           0.16267113,
           0.17639247,
           -0.0518755,
           0.1747323,
           -0.048224594,
           0.10348169,
           0.04488324,
           -0.20296545,
           -0.19428101,
           0.02257601,
           -0.15433174,
           -0.14458859,
           0.13768466,
           0.07455549,
           0.11423538,
           0.12939948,
           0.049330655,
           0.2198246,
           -0.19136792,
           -0.19512823,
           0.051436204,
           0.07025455,
           0.04913486,
           -0.050394952,
           0.04715927,
           -0.030752715,
           0.15692031,
           0.089972414,
           0.07153479,
           -0.11317301,
           -0.16075106,
           -0.04989604,
           -0.0010930029,
           -0.0211406,
           0.16605662,
           -0.03169356,
           -0.03405064,
           -0.10170521,
           -0.04605527,
           -0.2533857,
           0.028892186,
           -0.11631074,
           0.017622441,
           0.20009324,
           -0.0041399454,
           -0.09699936,
           -0.08047014,
           0.041251104,
           -0.08789679,
           -0.0038002464,
           0.07562178,
           0.08916414,
           -0.1783373,
           -0.090715565,
           0.064771384,
           -0.14538747,
           -0.012178358
          ],
          [
           0.021855038,
           0.13091521,
           0.010487399,
           -0.096136056,
           -0.040123224,
           -0.1644556,
           0.010501688,
           0.14435203,
           0.12873301,
           0.02890947,
           0.06345015,
           0.10410978,
           -0.23587582,
           0.03267421,
           0.09138061,
           0.09315418,
           0.08863629,
           -0.18128936,
           0.07917208,
           0.0087717585,
           0.009337167,
           0.10608395,
           0.126188,
           -0.08497579,
           -0.01875365,
           -0.13366571,
           0.11277825,
           -0.14599527,
           -0.17852144,
           0.21266367,
           -0.07574783,
           -0.007438466,
           -0.10868791,
           0.104284495,
           -0.18986142,
           0.003115486,
           -0.07845419,
           -0.14835383,
           -0.102821365,
           0.23317547,
           -0.124992654,
           -0.14730962,
           0.1574121,
           -0.019364916,
           -0.21375528,
           0.06062781,
           -0.03951754,
           -0.16714947,
           -0.060040012,
           0.1991762,
           -0.12850292,
           0.20234399,
           -0.10494125,
           0.00551293,
           0.15513699,
           -0.015876228,
           -0.0925104,
           0.07672764,
           0.15315677,
           0.060079638,
           -0.0018273136,
           -0.06408877,
           -0.036527492,
           -0.08781139,
           0.17124744,
           0.115002744,
           0.16162592,
           0.19217509,
           0.021036591,
           0.14458881,
           -0.07532977,
           0.17733368,
           -0.095643274,
           0.2194859,
           -0.06386242,
           0.14669661,
           -0.0720736,
           0.04392664,
           0.14917143,
           0.021209624,
           -0.08974223,
           -0.20410402,
           0.16916136,
           -0.1586997,
           0.16597638,
           0.09664388,
           0.033956945,
           0.1683228,
           -0.17023715,
           -0.0872561,
           0.022559058,
           -0.16696723,
           -0.21249777,
           0.017262781,
           -0.041225187,
           -0.12440415,
           0.07559211,
           0.081782885,
           0.10245034,
           0.14145271,
           -0.012098325,
           0.12672235,
           -0.08999727,
           -0.0918319,
           -0.08714917,
           -0.096262306,
           -0.10492615,
           -0.035759993,
           0.108053535,
           0.06835634,
           0.22698122,
           0.11459253,
           0.033783816,
           0.16728233,
           -0.06208351,
           -0.19922197,
           0.0013874037,
           -0.0571631,
           0.24972554,
           0.057222504,
           -0.14556086,
           -0.08525409,
           -0.22278294,
           0.03922637,
           0.034107335,
           -0.26633814,
           0.13675961,
           -0.05861561
          ],
          [
           0.062058035,
           -0.1674679,
           -0.0109997215,
           0.0291225,
           -0.07716963,
           0.06149928,
           -0.15882824,
           -0.01492317,
           -0.061899815,
           -0.09765624,
           -0.015788177,
           -0.12077139,
           0.03173699,
           -0.014459113,
           -0.12851365,
           -0.10619124,
           0.09839858,
           -0.08543104,
           0.09851733,
           0.011755194,
           -0.0113843065,
           0.16537113,
           -0.08438531,
           -0.09831719,
           -0.20393024,
           0.011812434,
           0.07397205,
           0.014723576,
           0.13152592,
           0.109675884,
           0.18655173,
           -0.18910259,
           0.12308479,
           0.083081976,
           0.09495377,
           -0.090509355,
           -0.09682751,
           0.07951899,
           0.21263155,
           0.14676382,
           0.19137517,
           0.14666812,
           -0.04049383,
           -0.06189192,
           -0.01495088,
           0.1759345,
           0.21645088,
           0.1495917,
           -0.058613162,
           -0.1114549,
           0.049110822,
           -0.010662804,
           -0.009031248,
           0.00827007,
           -0.0001782191,
           0.16384096,
           -0.08075261,
           0.011543901,
           0.1266488,
           0.018167863,
           0.001672775,
           0.13160463,
           0.23443101,
           0.23881577,
           0.014943736,
           -0.037152734,
           0.036731746,
           0.20466048,
           -0.10162577,
           0.085938245,
           -0.22816892,
           0.00625119,
           -0.08338389,
           0.0868461,
           -0.13286729,
           0.078104615,
           -0.07295756,
           0.14627162,
           0.037220147,
           0.020614052,
           0.19559468,
           0.08642202,
           -0.09986193,
           0.12369768,
           0.022779727,
           -0.07473396,
           0.15687668,
           -0.21154946,
           -0.09770638,
           -0.16120528,
           -0.07929698,
           0.113099866,
           -0.0040414436,
           -0.088979915,
           0.009528717,
           0.1552343,
           0.043653756,
           0.05299158,
           -0.026263244,
           0.12127628,
           0.022658348,
           0.21899436,
           0.19225438,
           -0.17587984,
           0.05393294,
           -0.16732946,
           -0.0139041385,
           0.059688017,
           -0.18956913,
           -0.09309297,
           0.03445843,
           -0.008363509,
           0.12007426,
           0.06207151,
           0.003405539,
           0.12776522,
           -0.0907491,
           -0.055624314,
           -0.12592487,
           0.080186896,
           -0.22947988,
           0.09755055,
           -0.057045076,
           -0.042750645,
           -0.07194078,
           0.10707843,
           -0.08482167,
           0.20960586
          ],
          [
           0.10558463,
           -0.046579506,
           -0.047351558,
           -0.07590731,
           -0.026148334,
           -0.05496934,
           -0.050849866,
           -0.20839117,
           -0.13801964,
           -0.038324106,
           0.048602656,
           0.041131537,
           -0.07026914,
           0.080795564,
           -0.07659923,
           -0.19493306,
           -0.22903505,
           0.05083834,
           -0.11959668,
           -0.05725078,
           0.15814002,
           -0.18643026,
           0.037130285,
           -0.020435896,
           0.21454778,
           0.09012772,
           0.07241005,
           -0.12239338,
           -0.11902597,
           0.071579464,
           -0.13884412,
           0.1721632,
           -0.035667673,
           0.096551314,
           0.0006474002,
           -0.009718365,
           0.15474899,
           0.031485233,
           0.11867668,
           0.02702239,
           0.014973688,
           0.113012634,
           -0.1631264,
           0.1077116,
           0.01762821,
           -0.024034237,
           0.022157524,
           -0.22784325,
           0.099607766,
           0.08621047,
           -0.04977268,
           0.16209853,
           0.14160582,
           -0.054110188,
           -0.12299731,
           -0.19243239,
           -0.017934041,
           -0.11194,
           0.057234418,
           -0.118337415,
           -0.056876183,
           -0.098018125,
           0.09304579,
           -0.11910624,
           -0.20938613,
           -0.18807189,
           -0.15136494,
           -0.05269251,
           0.21113174,
           0.073737636,
           0.078074306,
           0.19326742,
           -0.11599508,
           -0.15362652,
           -0.023841195,
           0.043600768,
           -0.22475614,
           -0.1718347,
           0.12502488,
           0.17356186,
           0.035306662,
           -0.0051160385,
           -0.22283766,
           0.11524312,
           0.1065319,
           -0.13057402,
           -0.09393086,
           -0.20275787,
           0.085883,
           -0.031497844,
           0.19008093,
           0.024569027,
           0.098914966,
           0.083800435,
           -0.08348168,
           -0.027624646,
           -0.028598145,
           0.06488348,
           -0.09233019,
           -0.16354986,
           0.09574741,
           0.20466983,
           -0.0461572,
           0.20311825,
           0.010493405,
           -0.107058704,
           -0.047074668,
           0.07472382,
           -0.06787043,
           0.031513624,
           0.17965655,
           0.23722813,
           -0.14782044,
           0.052517932,
           -0.07872101,
           0.047553577,
           -0.10944605,
           0.16803108,
           0.030577894,
           0.14964828,
           0.17207378,
           0.015172395,
           0.08046673,
           -0.16679598,
           0.052905336,
           -0.14233458,
           -0.25643003,
           0.017768336
          ],
          [
           -0.017713575,
           -0.027992602,
           -0.0651033,
           0.018021245,
           -0.0053361254,
           0.029427398,
           0.04057442,
           -0.074166566,
           0.05503801,
           0.14465474,
           0.117198795,
           0.04684535,
           -0.03719795,
           -0.0020485497,
           0.07403202,
           -0.101072475,
           0.016593877,
           -0.08508446,
           -0.013681056,
           -0.044716854,
           -0.014557661,
           0.16989115,
           0.055064037,
           0.009685718,
           -0.09123871,
           0.08366725,
           0.101321414,
           -0.12890556,
           -0.06322434,
           0.052210685,
           -0.03282919,
           -0.015310259,
           0.05450666,
           0.044239886,
           -0.016320614,
           0.07610529,
           -0.0033760977,
           0.008654808,
           -0.13882157,
           0.05377263,
           0.06821047,
           -0.044386804,
           -0.008719987,
           0.11956468,
           -0.007879025,
           -0.0013276624,
           0.0062902267,
           -0.044035167,
           -0.015886292,
           -0.017329939,
           -0.010806945,
           0.04325447,
           0.067215286,
           0.05262164,
           0.025537858,
           -0.03780093,
           0.04356892,
           0.009934953,
           0.113685615,
           -0.08660653,
           0.076710664,
           -0.080553114,
           0.0035521141,
           -0.07913527,
           0.074857116,
           -0.027014602,
           -0.031666897,
           0.00058431324,
           0.105251685,
           -0.07759035,
           -0.00268145,
           0.107759364,
           0.0149215525,
           0.09259795,
           -0.045590177,
           0.025197018,
           0.06353375,
           0.044963107,
           -0.029646304,
           -0.0025956258,
           -0.037730392,
           -0.09629863,
           0.023116099,
           -0.07073974,
           -0.063782506,
           -0.12419602,
           -0.048788596,
           0.047636114,
           0.01761872,
           0.07324348,
           -0.049899302,
           -0.010913979,
           -0.06824405,
           0.028715773,
           -0.050036896,
           -0.016448379,
           -0.019658037,
           -0.029038029,
           -0.09010996,
           -0.030251341,
           -0.14256383,
           0.05924153,
           0.0822343,
           -0.044175617,
           -0.12392146,
           0.16219802,
           -0.07393562,
           -0.035393994,
           0.021676335,
           0.10032905,
           -0.073303565,
           -0.049086437,
           0.00854257,
           0.07129522,
           0.15486883,
           0.03977054,
           -0.006329615,
           -0.12042276,
           -0.071109325,
           0.14202829,
           0.019931924,
           0.026921554,
           -0.07937199,
           -0.013007121,
           -0.0394012,
           -0.019776387,
           0.0048855585,
           0.020963846
          ],
          [
           -0.052084267,
           -0.09531737,
           -0.08831296,
           0.13306631,
           0.012958113,
           0.089737296,
           -0.05930928,
           -0.010601584,
           0.06166943,
           -0.01570762,
           -0.051318385,
           0.06279375,
           -0.10568667,
           -0.10566502,
           -0.018653195,
           0.06586324,
           0.023091527,
           -0.080839805,
           -0.00016533214,
           -0.030871239,
           -0.15064232,
           -0.12714913,
           -0.07284571,
           -0.07469848,
           -0.05780452,
           -0.05342751,
           -0.0696248,
           0.14196607,
           -0.046264295,
           -0.002452143,
           -0.04662601,
           0.049740437,
           0.0025318014,
           0.12504107,
           -0.021335287,
           0.027950905,
           -0.03920802,
           0.043053813,
           0.026371898,
           -0.16056038,
           -0.011730937,
           -0.022466553,
           -0.079086825,
           -0.057122793,
           -0.04116,
           -0.044795547,
           0.01874458,
           0.0033283613,
           -0.03626527,
           -0.022944251,
           0.026091617,
           -0.026873203,
           -0.062337328,
           0.0132892635,
           0.062415175,
           0.03943687,
           0.002199657,
           0.036558956,
           0.011324841,
           -0.07610898,
           -0.11013635,
           -0.017360045,
           0.071768016,
           0.030518757,
           -0.05266594,
           0.016831014,
           -0.045568123,
           -0.15246335,
           -0.06414629,
           0.000904397,
           -0.06566615,
           -0.14496541,
           0.061283868,
           0.105286025,
           0.01863342,
           0.044013534,
           0.009161544,
           -0.06322777,
           0.0028772405,
           -0.09259255,
           -0.10124736,
           -0.02550029,
           0.10404634,
           0.0024588136,
           0.024946876,
           -0.16772147,
           -0.057246037,
           0.13796754,
           0.07136999,
           0.22404344,
           0.117735475,
           0.10087395,
           0.037275545,
           0.05309929,
           0.042097583,
           -0.06182693,
           -0.17549738,
           -0.097554274,
           0.015701048,
           0.16897857,
           -0.1546829,
           0.022852914,
           0.02328171,
           0.10985962,
           -0.019513994,
           -0.05647103,
           0.08538093,
           0.13070004,
           0.09267439,
           -0.03813519,
           0.107727006,
           0.10605332,
           -0.012734579,
           -0.019344803,
           0.06892647,
           0.033357885,
           -0.1541841,
           0.0041907355,
           0.04464089,
           0.18624224,
           0.06738617,
           -0.044256225,
           -0.07858271,
           -0.07733402,
           0.0252006,
           -0.013922876,
           0.103778616,
           0.13224307
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "rgb(247,251,255)"
          ],
          [
           0.125,
           "rgb(222,235,247)"
          ],
          [
           0.25,
           "rgb(198,219,239)"
          ],
          [
           0.375,
           "rgb(158,202,225)"
          ],
          [
           0.5,
           "rgb(107,174,214)"
          ],
          [
           0.625,
           "rgb(66,146,198)"
          ],
          [
           0.75,
           "rgb(33,113,181)"
          ],
          [
           0.875,
           "rgb(8,81,156)"
          ],
          [
           1,
           "rgb(8,48,107)"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(model.embed.W_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding shape is torch.Size([11, 128]), so our vectors of length 128\n"
     ]
    }
   ],
   "source": [
    "# Take the dot product of all the embedding vectors\n",
    "emb = model.embed.W_E\n",
    "vec_count = emb.shape[0]\n",
    "vec_dim = emb.shape[1]\n",
    "print(f\"The embedding shape is {emb.shape}, so our vectors of length {emb.shape[1]}\")\n",
    "\n",
    "dot_products = einops.einsum(emb, emb, \"v2 embs, v1 emb -> v1 v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 11])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0.18764171,
           0.26676604,
           -0.8671944,
           -0.41341057,
           -0.082614675,
           -0.76576304,
           0.2362136,
           0.7582703,
           -0.41233948,
           0.08171892,
           -0.1719785
          ],
          [
           0.26676604,
           0.37925532,
           -1.2328709,
           -0.58773655,
           -0.11745144,
           -1.0886682,
           0.3358196,
           1.078016,
           -0.5862138,
           0.11617797,
           -0.244498
          ],
          [
           -0.8671944,
           -1.2328709,
           4.007777,
           1.910595,
           0.38180733,
           3.5390074,
           -1.0916716,
           -3.5043795,
           1.905645,
           -0.3776676,
           0.7948062
          ],
          [
           -0.41341057,
           -0.58773655,
           1.910595,
           0.91082245,
           0.18201591,
           1.6871223,
           -0.5204237,
           -1.6706144,
           0.90846264,
           -0.1800424,
           0.3789015
          ],
          [
           -0.082614675,
           -0.11745144,
           0.38180733,
           0.18201591,
           0.03637349,
           0.33714923,
           -0.103999846,
           -0.33385035,
           0.18154433,
           -0.03597911,
           0.07571849
          ],
          [
           -0.76576304,
           -1.0886682,
           3.5390074,
           1.6871223,
           0.33714923,
           3.1250675,
           -0.9639842,
           -3.0944898,
           1.6827512,
           -0.3334937,
           0.7018417
          ],
          [
           0.2362136,
           0.3358196,
           -1.0916716,
           -0.5204237,
           -0.103999846,
           -0.9639842,
           0.29735854,
           0.95455194,
           -0.51907533,
           0.10287222,
           -0.2164959
          ],
          [
           0.7582703,
           1.078016,
           -3.5043795,
           -1.6706144,
           -0.33385035,
           -3.0944898,
           0.95455194,
           3.0642114,
           -1.6662861,
           0.3302306,
           -0.6949744
          ],
          [
           -0.41233948,
           -0.5862138,
           1.905645,
           0.90846264,
           0.18154433,
           1.6827512,
           -0.51907533,
           -1.6662861,
           0.906109,
           -0.17957594,
           0.37791982
          ],
          [
           0.08171892,
           0.11617797,
           -0.3776676,
           -0.1800424,
           -0.03597911,
           -0.3334937,
           0.10287222,
           0.3302306,
           -0.17957594,
           0.035589006,
           -0.07489751
          ],
          [
           -0.1719785,
           -0.244498,
           0.7948062,
           0.3789015,
           0.07571849,
           0.7018417,
           -0.2164959,
           -0.6949744,
           0.37791982,
           -0.07489751,
           0.15762275
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dot_products.shape)\n",
    "imshow_div(dot_products)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What would your hypothesis around the attention head activations be based on seeing this?\n",
    "+ Jack - My poorly informed guess is that tokens with low dot products and/or low norms won't have any strong attentional interaction\n",
    "+ Omar - I think that corner moves [0, 2, 6, 8] will have similar attention patterns\n",
    "+ Ari - I think same as Omar, plus center attends to everything, middle edges have attention symmetry too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [0,1,2,3,4,6,5,8,7]\n",
    "# tokens = ([10] * 5) + [1,2,5,8,7]\n",
    "str_tokens = [str(token) for token in tokens]\n",
    "logits, cache = model.run_with_cache(torch.tensor(tokens).to('cuda'), remove_batch_dim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformer_lens.ActivationCache.ActivationCache'>\n",
      "torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(type(cache))\n",
    "attention_pattern = cache[\"pattern\", 0, \"attn\"]\n",
    "print(attention_pattern.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-057d27ec-50cd\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-057d27ec-50cd\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"10\", \"10\", \"10\", \"10\", \"10\", \"1\", \"2\", \"5\", \"8\", \"7\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6598382592201233, 0.3401618003845215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4664892256259918, 0.25497716665267944, 0.27853357791900635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3350974917411804, 0.22458519041538239, 0.2163139134645462, 0.22400341928005219, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23025262355804443, 0.19681333005428314, 0.19143056869506836, 0.18791531026363373, 0.19358821213245392, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1376420259475708, 0.2018892616033554, 0.18153470754623413, 0.16982662677764893, 0.15761658549308777, 0.15149079263210297, 0.0, 0.0, 0.0, 0.0], [0.09861132502555847, 0.17011789977550507, 0.15474052727222443, 0.1381695717573166, 0.13176923990249634, 0.17606191337108612, 0.13052956759929657, 0.0, 0.0, 0.0], [0.052961964160203934, 0.14475230872631073, 0.13106343150138855, 0.11304518580436707, 0.10282398015260696, 0.18627174198627472, 0.1581994891166687, 0.11088185757398605, 0.0, 0.0], [0.025211507454514503, 0.11672952771186829, 0.10326165705919266, 0.08920213580131531, 0.07686608284711838, 0.2255043387413025, 0.17721326649188995, 0.12884879112243652, 0.05716269835829735, 0.0], [0.058662354946136475, 0.11316423863172531, 0.09410063177347183, 0.07788167893886566, 0.07835467904806137, 0.11499825865030289, 0.11203068494796753, 0.11006581783294678, 0.11249109357595444, 0.12825055420398712]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f2bc77837f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
