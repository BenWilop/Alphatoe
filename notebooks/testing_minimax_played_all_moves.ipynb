{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from alphatoe import evals, data, game, train\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "import json\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from typing import Callable, Any\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating all possible games...\n",
      "Generated 255168 games\n",
      "Generated array of moves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255168/255168 [01:16<00:00, 3317.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([255168, 10, 10])\n",
      "Generated all data and minimax labels\n",
      "torch.Size([255168, 10, 10])\n",
      "torch.Size([255168, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels = data.gen_data('minimax all', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([204134, 10, 10])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 4,\n",
    "    d_model = 16,\n",
    "    d_head = 4,\n",
    "    d_mlp = 64,\n",
    "    act_fn = \"relu\",\n",
    "    #normalization_type=None,\n",
    "    normalization_type='LN',\n",
    "    d_vocab=11,\n",
    "    d_vocab_out=10,\n",
    "    n_ctx=10,\n",
    "    init_weights=True,\n",
    "    device=\"cuda\",\n",
    "    seed = 1337,\n",
    ")\n",
    "\n",
    "lr = 1e-5\n",
    "weight_decay = 1e-4\n",
    "test_train_split = 0.7\n",
    "epochs = 10_000\n",
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer(cfg).to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = cross_entropy\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.4147329330444336 | Test Loss: 2.4136805534362793\n",
      "Epoch 1 | Train Loss: 2.257502555847168 | Test Loss: 2.2587597370147705\n",
      "Epoch 2 | Train Loss: 2.129323959350586 | Test Loss: 2.1316447257995605\n",
      "Epoch 3 | Train Loss: 2.035799503326416 | Test Loss: 2.0383479595184326\n",
      "Epoch 4 | Train Loss: 1.973514199256897 | Test Loss: 1.9756914377212524\n",
      "Epoch 5 | Train Loss: 1.9300286769866943 | Test Loss: 1.931776523590088\n",
      "Epoch 6 | Train Loss: 1.897434949874878 | Test Loss: 1.898775577545166\n",
      "Epoch 7 | Train Loss: 1.8707481622695923 | Test Loss: 1.871781587600708\n",
      "Epoch 8 | Train Loss: 1.8469898700714111 | Test Loss: 1.8478059768676758\n",
      "Epoch 9 | Train Loss: 1.8250296115875244 | Test Loss: 1.8256806135177612\n",
      "Epoch 10 | Train Loss: 1.8041012287139893 | Test Loss: 1.804573655128479\n",
      "Epoch 11 | Train Loss: 1.7838408946990967 | Test Loss: 1.7841761112213135\n",
      "Epoch 12 | Train Loss: 1.7641794681549072 | Test Loss: 1.764359474182129\n",
      "Epoch 13 | Train Loss: 1.7447607517242432 | Test Loss: 1.7447941303253174\n",
      "Epoch 14 | Train Loss: 1.7252540588378906 | Test Loss: 1.7251461744308472\n",
      "Epoch 15 | Train Loss: 1.7055456638336182 | Test Loss: 1.7052525281906128\n",
      "Epoch 16 | Train Loss: 1.6859791278839111 | Test Loss: 1.6854472160339355\n",
      "Epoch 17 | Train Loss: 1.6663360595703125 | Test Loss: 1.6655575037002563\n",
      "Epoch 18 | Train Loss: 1.646681308746338 | Test Loss: 1.6456761360168457\n",
      "Epoch 19 | Train Loss: 1.627195119857788 | Test Loss: 1.625914454460144\n",
      "Epoch 20 | Train Loss: 1.6078670024871826 | Test Loss: 1.6063663959503174\n",
      "Epoch 21 | Train Loss: 1.5889849662780762 | Test Loss: 1.5874046087265015\n",
      "Epoch 22 | Train Loss: 1.5706992149353027 | Test Loss: 1.5691401958465576\n",
      "Epoch 23 | Train Loss: 1.5533188581466675 | Test Loss: 1.551835298538208\n",
      "Epoch 24 | Train Loss: 1.5368785858154297 | Test Loss: 1.535456657409668\n",
      "Epoch 25 | Train Loss: 1.5210351943969727 | Test Loss: 1.5196735858917236\n",
      "Epoch 26 | Train Loss: 1.5057741403579712 | Test Loss: 1.5044623613357544\n",
      "Epoch 27 | Train Loss: 1.4908108711242676 | Test Loss: 1.4895516633987427\n",
      "Epoch 28 | Train Loss: 1.4761216640472412 | Test Loss: 1.474884271621704\n",
      "Epoch 29 | Train Loss: 1.46197509765625 | Test Loss: 1.4607969522476196\n",
      "Epoch 30 | Train Loss: 1.4487076997756958 | Test Loss: 1.4475001096725464\n",
      "Epoch 31 | Train Loss: 1.436362624168396 | Test Loss: 1.4350996017456055\n",
      "Epoch 32 | Train Loss: 1.4250760078430176 | Test Loss: 1.4237862825393677\n",
      "Epoch 33 | Train Loss: 1.41461980342865 | Test Loss: 1.413388967514038\n",
      "Epoch 34 | Train Loss: 1.4048559665679932 | Test Loss: 1.4037632942199707\n",
      "Epoch 35 | Train Loss: 1.3957308530807495 | Test Loss: 1.3947875499725342\n",
      "Epoch 36 | Train Loss: 1.387285590171814 | Test Loss: 1.3865160942077637\n",
      "Epoch 37 | Train Loss: 1.3794804811477661 | Test Loss: 1.3788901567459106\n",
      "Epoch 38 | Train Loss: 1.3721224069595337 | Test Loss: 1.3716963529586792\n",
      "Epoch 39 | Train Loss: 1.3652421236038208 | Test Loss: 1.3649654388427734\n",
      "Epoch 40 | Train Loss: 1.358779788017273 | Test Loss: 1.358656883239746\n",
      "Epoch 41 | Train Loss: 1.3526755571365356 | Test Loss: 1.352726697921753\n",
      "Epoch 42 | Train Loss: 1.3468945026397705 | Test Loss: 1.3471397161483765\n",
      "Epoch 43 | Train Loss: 1.3414039611816406 | Test Loss: 1.3417960405349731\n",
      "Epoch 44 | Train Loss: 1.33613121509552 | Test Loss: 1.336666464805603\n",
      "Epoch 45 | Train Loss: 1.3310339450836182 | Test Loss: 1.331743597984314\n",
      "Epoch 46 | Train Loss: 1.3261386156082153 | Test Loss: 1.3269902467727661\n",
      "Epoch 47 | Train Loss: 1.3214205503463745 | Test Loss: 1.322399377822876\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_minimax_played_all_moves.ipynb Cell 7\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_minimax_played_all_moves.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_loss \u001b[39m=\u001b[39m loss_fn(rearranged_train_logits,rearranged_train_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_minimax_played_all_moves.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_minimax_played_all_moves.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_losses\u001b[39m.\u001b[39mappend(train_loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_minimax_played_all_moves.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_minimax_played_all_moves.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "# for epoch in tqdm.tqdm(range(epochs)):\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, len(train_data), batch_size):\n",
    "        t_logits = train_data[batch:batch+batch_size]\n",
    "        t_labels = train_labels[batch:batch+batch_size]\n",
    "        rearranged_train_logits = einops.rearrange(model(t_logits), \"batch seq_len one_hots -> (batch seq_len) one_hots\")\n",
    "        rearranged_train_labels = einops.rearrange(t_labels, \"batch seq_len one_hots -> (batch seq_len) one_hots\")\n",
    "        train_loss = loss_fn(rearranged_train_logits,rearranged_train_labels)\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            test_logits = model(test_data)\n",
    "\n",
    "            rearranged_test_logits = einops.rearrange(test_logits, \"batch seq_len one_hots -> (batch seq_len) one_hots\")\n",
    "            rearranged_test_labels = einops.rearrange(test_labels, \"batch seq_len one_hots -> (batch seq_len) one_hots\")\n",
    "\n",
    "            test_loss = loss_fn(rearranged_test_logits, rearranged_test_labels)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss.item()} | Test Loss: {test_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
