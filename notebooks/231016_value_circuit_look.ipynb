{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from alphatoe import plot, game, interpretability\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import json\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = interpretability.load_model(\n",
    "    \"../scripts/models/prob all 8 layer control-20230718-185339\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_attention(seq: torch.Tensor) -> torch.Tensor:\n",
    "    def hook(module, input, output):\n",
    "        module.attention = output.clone()\n",
    "    try:\n",
    "        handle = model.blocks[0].hook_attn_out.register_forward_hook(hook)\n",
    "        _ = model(seq)\n",
    "        attention = model.blocks[0].hook_attn_out.attention\n",
    "    except Exception as e:\n",
    "        handle.remove()\n",
    "        raise e\n",
    "    \n",
    "    return attention\n",
    "\n",
    "def get_value_attention(seq: torch.Tensor) -> torch.Tensor:\n",
    "    def hook(module, input, output):\n",
    "        module.value = output.clone()\n",
    "    try:\n",
    "        handle = model.blocks[0].attn.hook_v.register_forward_hook(hook)\n",
    "        _ = model(seq)\n",
    "        value = model.blocks[0].attn.hook_v.value\n",
    "    except Exception as e:\n",
    "        handle.remove()\n",
    "        raise e\n",
    "    \n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2119,  0.0599, -0.4458,  0.6086, -0.2821,  0.1315, -0.0441,  0.1153,\n",
       "        -0.1475,  0.7113,  0.3539, -0.2458,  0.3536,  0.3840, -0.4710,  0.6980],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#win top left to bottom right\n",
    "moves = [10, 0, 10, 4, 10, 8]\n",
    "seq = torch.tensor(moves)\n",
    "get_value_attention(seq)[0, -1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = model.pos_embed(seq.unsqueeze(0))[0]\n",
    "pos[0] = 0\n",
    "pos[2] = 0\n",
    "pos[4] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_two_wv = model.blocks[0].attn.W_V[2]\n",
    "\n",
    "pos_value = pos @ head_two_wv\n",
    "pos_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_two_wo = model.blocks[0].attn.W_O[2]\n",
    "\n",
    "pos_out = pos_value @ head_two_wo\n",
    "pos_out_last = pos_out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_attention(seq: torch.Tensor) -> torch.Tensor:\n",
    "    def hook(module, input, output):\n",
    "        module.attention = output.clone()\n",
    "\n",
    "    try:\n",
    "        handle = model.blocks[0].hook_attn_out.register_forward_hook(hook)\n",
    "        _ = model(seq)\n",
    "        attention = model.blocks[0].hook_attn_out.attention\n",
    "        handle.remove()\n",
    "    except Exception as e:\n",
    "        handle.remove()\n",
    "        raise e\n",
    "\n",
    "    return attention\n",
    "\n",
    "def modify_pre_mlp_residuals(seq: torch.Tensor, vec) -> torch.Tensor:\n",
    "    def hook(module, input, output):\n",
    "        return vec\n",
    "\n",
    "    try:\n",
    "        handle = model.blocks[0].hook_attn_out.register_forward_hook(hook)\n",
    "        out = model(seq)\n",
    "        handle.remove()\n",
    "    except Exception as e:\n",
    "        handle.remove()\n",
    "        raise e\n",
    "\n",
    "    return out\n",
    "\n",
    "def get_activation_vector(move: int):\n",
    "    moves = list(range(0, 9))\n",
    "    moves.remove(move)\n",
    "    z_seqs = [[10, move, snd] for snd in moves]\n",
    "    other_seqs = [[[10, fst, snd] for fst in moves if fst != snd] for snd in moves]\n",
    "    all_acts = []\n",
    "    flatten_acts = []\n",
    "    for i, z_seq in enumerate(z_seqs):\n",
    "        z_act = get_head_attention((torch.tensor(z_seq)))[0, -1]\n",
    "        z_acts = []\n",
    "        for other_seq in other_seqs[i]:\n",
    "            other_act = get_head_attention((torch.tensor(other_seq)))[0, -1]\n",
    "\n",
    "            act_diff = other_act - z_act\n",
    "            z_acts.append(act_diff)\n",
    "            flatten_acts.append(act_diff)\n",
    "\n",
    "        all_acts.append(z_acts)\n",
    "    flat_acts = torch.stack(flatten_acts)\n",
    "    # return torch.norm(flat_acts, dim=1)\n",
    "    return flat_acts.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_act = get_activation_vector(0)\n",
    "one_act = get_activation_vector(1)\n",
    "two_act = get_activation_vector(2)\n",
    "three_act = get_activation_vector(3)\n",
    "four_act = get_activation_vector(4)\n",
    "five_act = get_activation_vector(5)\n",
    "six_act = get_activation_vector(6)\n",
    "seven_act = get_activation_vector(7)\n",
    "eight_act = get_activation_vector(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-21.5328,  10.4674,   8.3215,  10.3605, -20.5882,   8.9510,   8.2554,\n",
      "         10.0498, -25.0590,   4.7009], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-21.0829,  10.5153,   8.8922,  10.4121, -19.8378,   9.0687,   8.7273,\n",
      "         10.9109, -23.8312,   1.3638], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 1.3368,  2.4447, -0.5984,  1.6671,  0.1068,  2.8515,  0.1042,  0.7622,\n",
      "        -5.2992, -3.0658], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vec = pos_out_last - zero_act - four_act - eight_act\n",
    "no_pos_vec = zero_act + four_act + eight_act\n",
    "print(modify_pre_mlp_residuals(seq, vec)[0,-1])\n",
    "print(modify_pre_mlp_residuals(seq, -no_pos_vec)[0,-1])\n",
    "print(modify_pre_mlp_residuals(seq, pos_out_last)[0,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_content_embedding(seq: torch.Tensor) -> torch.Tensor:\n",
    "    def hook(module, input, output):\n",
    "        return torch.zeros_like(output)\n",
    "    def other_hook(module, input, output):\n",
    "        module.attention = output.clone()\n",
    "    try:\n",
    "        handle = model.hook_embed.register_forward_hook(hook)\n",
    "        other_handle = model.blocks[0].hook_attn_out.register_forward_hook(other_hook)\n",
    "        _ = model(seq)\n",
    "        attention = model.blocks[0].hook_attn_out.attention\n",
    "        handle.remove()\n",
    "        other_handle.remove()\n",
    "    except Exception as e:\n",
    "        handle.remove()\n",
    "        raise e\n",
    "\n",
    "    return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_content = zero_content_embedding(seq)[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-32.4057,  14.9805, -25.8678,  17.5184,  18.7365,  14.3305,  15.0063,\n",
      "         11.6420, -30.4062,  -3.6964], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-50.1820,  18.9277, -14.2695,  58.7326,  70.8523,  17.0660,  36.7576,\n",
      "        -13.9224, -51.8323, -87.3968], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vec = zero_content - zero_act - two_act - eight_act\n",
    "strong_vec = 10*zero_content - zero_act - two_act - eight_act\n",
    "print(modify_pre_mlp_residuals(seq, vec)[0,-1])\n",
    "print(modify_pre_mlp_residuals(seq, strong_vec)[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-32.7298,  10.3781,   8.3182,   9.1393, -27.6169,   9.5415,   7.9608,\n",
       "          9.1062, -29.3370,  17.7020], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = interpretability.load_model(\n",
    "    \"../scripts/models/prob all 8 layer control-20230718-185339\"\n",
    ")\n",
    "#moves = [10, 0, 10, 4, 10, 8]\n",
    "model(seq)[0,-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
