{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphatoe import models, plot, interpretability, game\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_memlab import LineProfiler, MemReporter\n",
    "from showmethetypes import SMTT\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogFormatter\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = SMTT(\"numpy\", \"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc = models.SparseAutoEncoder(512, 1024).to(\"cuda\")\n",
    "\n",
    "loss_fn = torch.nn.functional.mse_loss\n",
    "optimizer = torch.optim.Adam(autoenc.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_data = torch.load(\"all_games_act_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor (dtype: torch.float32)\n",
      "    |  (device: cuda:0)\n",
      "    |__dim_0 (2361456)\n",
      "    |__dim_1 (512)\n"
     ]
    }
   ],
   "source": [
    "tt(act_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6646, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_data[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = loss_fn(torch.zeros(2, 2), torch.ones(2, 2), reduction=\"none\")\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a sparser encoder! (actually following instructions)\n",
    "- L0 around 10 or 20 on average across 1000 games\n",
    "- feature density is mostly under 1%\n",
    "- reconstruction loss stays low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, act_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], batch_size):\n\u001b[1;32m      7\u001b[0m     dat \u001b[39m=\u001b[39m act_data[batch : batch \u001b[39m+\u001b[39m batch_size]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     l0, reg, guess \u001b[39m=\u001b[39m autoenc(dat)\n\u001b[1;32m     10\u001b[0m     mse_loss \u001b[39m=\u001b[39m loss_fn(guess, dat)\n\u001b[1;32m     12\u001b[0m     sparse_loss \u001b[39m=\u001b[39m lam \u001b[39m*\u001b[39m reg\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/alphatoe/models.py:27\u001b[0m, in \u001b[0;36mSparseAutoEncoder.forward\u001b[0;34m(self, input, pt)\u001b[0m\n\u001b[1;32m     25\u001b[0m acts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlinearity(\u001b[39minput\u001b[39m \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_in \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_in)\n\u001b[1;32m     26\u001b[0m l1_regularization \u001b[39m=\u001b[39m acts\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39msum()  \u001b[39m# / self.hidden_dim\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m l0 \u001b[39m=\u001b[39m (acts \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49msum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mmean(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m pt:\n\u001b[1;32m     29\u001b[0m     \u001b[39mprint\u001b[39m(acts)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 2**15\n",
    "lam = 1e-7\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, act_data.shape[0], batch_size):\n",
    "        dat = act_data[batch : batch + batch_size].to(\"cuda\")\n",
    "\n",
    "        l0, reg, guess = autoenc(dat)\n",
    "        mse_loss = loss_fn(guess, dat)\n",
    "\n",
    "        sparse_loss = lam * reg\n",
    "        # sparse_loss = 0\n",
    "        loss = mse_loss + sparse_loss\n",
    "        # losses.append(interpretability.numpy(loss))\n",
    "        losses.append([mse_loss.item(), sparse_loss.item(), l0.item()])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(losses[-1])\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_loss = loss_fn(guess, dat, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[5.3487e-09, 7.1918e-09, 3.5960e-08,  ..., 2.6246e-01, 2.6246e-01,\n",
       "         2.8048e-01],\n",
       "        [4.6919e-07, 6.7657e-07, 1.9079e-06,  ..., 1.0078e+00, 1.0573e+00,\n",
       "         1.2321e+00],\n",
       "        [8.9489e-07, 3.3453e-06, 1.2714e-05,  ..., 6.7506e-01, 7.4010e-01,\n",
       "         1.2074e+00],\n",
       "        ...,\n",
       "        [2.1759e-07, 4.2643e-07, 5.7342e-06,  ..., 4.2370e-01, 4.3755e-01,\n",
       "         6.2129e-01],\n",
       "        [1.9072e-06, 2.4738e-06, 3.0429e-06,  ..., 5.2393e-01, 5.3203e-01,\n",
       "         5.3374e-01],\n",
       "        [1.9040e-10, 1.1574e-06, 1.2861e-06,  ..., 2.5548e-01, 2.7523e-01,\n",
       "         3.1692e-01]], device='cuda:0'),\n",
       "indices=tensor([[508, 314, 184,  ..., 393,   7, 168],\n",
       "        [410, 507, 110,  ..., 285, 461, 308],\n",
       "        [213, 335, 320,  ..., 135, 209, 368],\n",
       "        ...,\n",
       "        [191, 443, 265,  ..., 348, 406, 243],\n",
       "        [265, 289, 176,  ...,  97, 158, 252],\n",
       "        [ 40, 483, 242,  ..., 209, 153, 237]], device='cuda:0'))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_loss.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/obayyub/p/Tic-Tac-Transformer/notebooks/231026_training_sparse_autoencoder_on_activations.ipynb Cell 36\u001b[0m line \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/231026_training_sparse_autoencoder_on_activations.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39myscale(\u001b[39m\"\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/231026_training_sparse_autoencoder_on_activations.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39;49mplot(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(losses)), losses)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/pyplot.py:2769\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2767\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2768\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2769\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2770\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2771\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1632\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1633\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1634\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    311\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 312\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(this, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/axes/_base.py:488\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(xy) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    487\u001b[0m     x \u001b[39m=\u001b[39m _check_1d(xy[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 488\u001b[0m     y \u001b[39m=\u001b[39m _check_1d(xy[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    489\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     x, y \u001b[39m=\u001b[39m index_of(xy[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1306\u001b[0m, in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1304\u001b[0m x \u001b[39m=\u001b[39m _unpack_to_numpy(x)\n\u001b[1;32m   1305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1306\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49matleast_1d(x)\n\u001b[1;32m   1307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1308\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m ary \u001b[39min\u001b[39;00m arys:\n\u001b[0;32m---> 65\u001b[0m     ary \u001b[39m=\u001b[39m asanyarray(ary)\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m ary\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     67\u001b[0m         result \u001b[39m=\u001b[39m ary\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:1030\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   1029\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m   1031\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.yscale(\"log\")\n",
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we've got an autoencoder, what do we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_freqs(num_batches=25, local_encoder=None):\n",
    "    if local_encoder is None:\n",
    "        local_encoder = encoder\n",
    "    act_freq_scores = torch.zeros(\n",
    "        local_encoder.W_in.shape[1], dtype=torch.float32\n",
    "    ).cuda()\n",
    "    total = 0\n",
    "    for i in tqdm.trange(num_batches):\n",
    "        tokens = act_data[torch.randperm(len(act_data))][: 2**14].to(\"cuda\")\n",
    "\n",
    "        hidden = local_encoder.get_act_density(tokens)\n",
    "\n",
    "        act_freq_scores += hidden\n",
    "        total += tokens.shape[0]\n",
    "    act_freq_scores /= total\n",
    "    num_dead = (act_freq_scores == 0).float().mean()\n",
    "    print(\"Num dead\", num_dead)\n",
    "    return act_freq_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:23<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead tensor(0.1621, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "freqs = get_freqs(local_encoder=autoenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor (dtype: torch.float32)\n",
      "    |  (device: cuda:0)\n",
      "    |__dim_0 (1024)\n"
     ]
    }
   ],
   "source": [
    "tt(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2202.3345, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(freqs[112] * act_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = interpretability.numpy(freqs) * act_data.shape[0]\n",
    "# x = interpretability.numpy(freqs)\n",
    "x = x[np.isfinite(x)]\n",
    "fig, ax = plt.subplots()\n",
    "# set figure size\n",
    "fig.set_size_inches(10, 6)\n",
    "ax.hist(x, bins=np.logspace(np.log10(5), np.log10(10000000), 100))\n",
    "ax.set_xscale(\"log\")\n",
    "# x label\n",
    "# ax.xlabel(\"Number of Moves (log 10 scale)\");\n",
    "# y label\n",
    "# ax.ylabel(\"Count of Features(neuron acts)\");\n",
    "# set xtick and labels of ticks\n",
    "tick_positions = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "tick_labels = [\"1\", \"10\", \"100\", \"1k\", \"10k\", \"100k\", \"1M\"]\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_xticklabels(tick_labels)\n",
    "# ax.get_xaxis().set_major_formatter(plt.ScalarFormatter());\n",
    "ax.set_xlabel(\"Number of Times Fired Out of 2,361,456\")\n",
    "ax.set_ylabel(\"Count of Features(neuron acts)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoenc, \"sparse_autoencoder_on_activations_02NOV2023.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
